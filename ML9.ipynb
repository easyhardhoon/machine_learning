{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObahFyIWjecMmyYroQu5jI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easyhardhoon/machine_learning/blob/master/ML9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "활성화 함수로 이루어진 신경망 계층들.\n",
        "\n",
        "계층이란 노드(동그라미)를 의미함. \n",
        "\n",
        "예를 들어 relu 계층은  >-- x --> RELU >-- y --> 이런식임\n",
        "\n",
        "추가 : 계층은 최대한 단순화된 노드의 집합이라고 이해해도 될듯\n",
        "\n",
        "relu는 계산이 더 이상 나누어지지 않아 relu가 하나의 노드처럼 인식\n",
        "\n",
        "sigmoid는 계산이 여러개로 세분화됨. 그 여러개가 각각의 노드이고\n",
        "\n",
        "sigmoid 계층은 이러한 노드들의 집합\n",
        "\n",
        "#결국은 sigmoid도 +, * , RELU 처럼 하나의 노드처럼 정리 가능  \n",
        "\n",
        "# 2. sigmoid 계층\n",
        "\n",
        "sigmoid함수 y = 1/1+exp(-x)\n",
        "\n",
        "세분화된 노드들로 순전파와 역전파를 구해보면, \n",
        "\n",
        "역전파의 결과물이 순전파의 입력 x와 출력 y만으로 계산할 수 있다\n",
        "\n",
        "또 그 식을 정리하면 역전파에서 dL/dy가 들어오면 dL/dy * y(1-y) 로 정리가능\n",
        "\n",
        "즉 위 relu처럼 하나의 sigmoid 노드로 정리할 수 있다. \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xc1BC_8-kALK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LxJuB0Loj8Ur"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None #아무튼간에 순전파떄의 출력 y가 역전파떄 쓰이기 떄문에 y를 기록해놔야함\n",
        "  def forward(self,x):\n",
        "    out = 1 / (1 + np.exp(-x))\n",
        "    self.out = out \n",
        "    return out\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "    return dx\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#신경망 순전파의 흐름\n",
        "\n",
        "▶ 1. Y = np.dot(X,W) + B\n",
        "\n",
        "▶ 2. Y = 활성화함수(Y). \n",
        "\n",
        "▶ 3. 다음 층으로 전파\n",
        "\n",
        "여기서 (1)의 행렬곱을 어파인 변환이라고 함. 어파인 변환을 수행하는 처리를\n",
        "\n",
        "(1)의 과정을 \"Affine 계층\" 이라고 부른다\n",
        "\n",
        "# 3. Affine 계층\n",
        "\n",
        "변수가 행렬(다차원배열)이긴 하지만 행렬의 원소마다 전개해보면 스칼라값을 이용했던 지금까지의 계산 그래프와 같은 순서이다.\n",
        "\n",
        "핵심은 : 변수가 행렬이기 때문에 , 행렬에 대응하는 차원의 원소 수가 일치하도록 곱을 조립하는 것이다 (특히 행렬곱)\n",
        "\n",
        "\n",
        "\n",
        "#Affine 계산 그래프에서\n",
        "\n",
        "X와 dL/dx는 같은 형상임. 순방/역전파의 개념 이해. \n",
        "\n",
        "마찬가지로 W와 dL/dw도 같은 형상\n",
        "\n",
        "W_t의 t는 전치행렬을 의미. 전치행렬은 W의 (i,j) 를 (j,i)로 바꿔줌\n",
        "\n",
        "**행렬의 곱에서 행렬의 형상을 맞춰줘야 하는 원리에 의해**,\n",
        "\n",
        "W_t 같은 전치행렬이 도입된다는 점을 이해\n",
        "\n",
        "ex) dL/dY 도트 (???) = dL/dX ,  ???에 W가 와야하는 상황\n",
        "\n",
        "-->  (3, ) 도트 (???) = ( 2,)  . 근데 W의 형상은 (2,3)이다\n",
        "\n",
        "#기본적으로 (3,)을 (1,3)으로 이해하자. \n",
        "\n",
        "???에는 (3,2)가 와야함을 알 수 있다 \n",
        "\n",
        "그래서 W의 형상을 뒤집은 W**T, 즉 전치행령이 도입\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SoxYNgjUtrFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#입력값 딜레마\n",
        "\n",
        "데이터가 1개든 여러개든 28*28크기로 인해 행렬의 느낌으로 \"배열\" 형식이 들어가서, 헷갈릴 수 있다.\n",
        "\n",
        "위에서 나오듯, \"데이터 1개로 된 신경망\" 과   \"데이터 n개, 즉 배치용 신경망\"이 따로 있다고 생각하면 편할 듯하다. \n",
        "\n",
        "즉 데이터가 하나가 들어가든 배치형식으로 한꺼번에 여러개가 들어가든\n",
        "입력형식은 행렬이다.\n",
        "\n",
        "하나이면 1차원 행렬, 배치이면 2차원 행렬 \n",
        "\n",
        "자세한건 잠시 뒤에 다시 나올듯\n",
        "\n",
        "#배치용 Affine 계층\n",
        "\n",
        "지금까지의 affine 계층은 입력 데이터로 X하나만을 고려한 것이었다.\n",
        "\n",
        "그런데 데이터를 N개 묶어 순전파하는 경우, 즉 배치용 Affine 계층을 생각해본다.\n",
        "\n",
        "\n",
        "배치용 Affine 계층과 입력 데이터 1개 Affine 계층의 차이\n",
        "\n",
        "--> 입력인 X의 형상이 (2,) 에서 (N,2)로 되었다. \n",
        "\n",
        "--> 입력 데이터 1개이나 배치용이나 기본적으로 배열(행렬)형식인건 맞다.\n",
        "\n",
        "(지금까지의 계산 그래프에는 노드 사이에 스칼라값이 들어갔었음)\n",
        "\n",
        "배치용이면 데이터의 차원이 추가되어 2차원 배열이 되는 것 뿐이다"
      ],
      "metadata": {
        "id": "lhaiRVycDCzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#배치용 Affine 계층에서의 편향에 대하여\n",
        "\n",
        "Y = np.dot(X,W) + B ------> 이 과정이 Affine 과정입니다.\n",
        "\n",
        " 여기서 \"+\" 노드에 주목하자\n",
        "\n",
        "\n",
        "순전파의 편향 덧셈은 각각의 데이터에 더해진다. \n",
        "\n",
        "( ex) 2차원 배열에 1차원 배열이 더해지는 꼴. numpy의 브로드캐스트 기능 덕분에 구현이 가능하다 )\n",
        "\n",
        "**그래서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 \"모여야\" 한다**\n",
        "\n",
        "---> \"+\"노드는 역전파때 입력값을 그대로 출력한다. \n",
        "\n",
        "--->그런데 입력값이 2차원인데 출력(편향)은 1차원이니까, 이 2차원값을 1차원 형식으로 바꿔준다는 뜻이다\n",
        "\n",
        "---> 2차원 원소들을 1차원으로 \"모은다\"의 느낌으로 이해하자"
      ],
      "metadata": {
        "id": "_eh5yRb8N0xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dY = np.array([[1,2,3], [4,5,6]])\n",
        "dB = np.sum(dY, axis=0) # + 노드의 역전파이니 입력값을 그대로 출력해야하는데 차원이 2차원에서 1차원으로 줄여야함\n",
        "#두 데이터에 대한 미분 (입력값)을 데이터마다 더해서 구하는 느낌. \n",
        "#그래서 np.sum에서 0번쨰 축(데이터 단위)에 대해 총합을 구했음.\n",
        "print(dY)\n",
        "print(dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7KTHlpUtZ5t",
        "outputId": "03f0ed80-94ca-4d20-efac-c0425f192723"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "[5 7 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "  def __init__(self,W,b):\n",
        "    self.W = W #역전파떄 필요\n",
        "    self.b = b \n",
        "    self.x = None #역전파때 필요\n",
        "    self.dw = None \n",
        "    self.db = None \n",
        "    # 나머지들은 안쓰이는데 왜 따로 저장하지..?\n",
        "    # 나중에 쓰일듯!\n",
        "  def forward(self, x):\n",
        "    #x는 입력데이터\n",
        "    self.x = x\n",
        "    out = np.dot(x,self.W) + self.b\n",
        "    return out\n",
        "  def backward(self,dout):\n",
        "    #최종 갈래는 3갈래. dx, dw, db\n",
        "    #아래 이상한 코드는 dot 노드의 역전파의 공식입니다. \n",
        "    dx  = np.dot(dout, self.W.T) #이유 : +역전파는 입력값 그대로고 도트 역전파는 dot&전치행렬의 개념이기 떄문\n",
        "    self.dW = np.dot(self.x.T, dout) #도트 역전파의 공식입니다. 순전파 입력값의 전치행렬에 dout의 도트를 한게 공식입니다.\n",
        "    self.db = np.sum(dout, axis=0)  #앞서 말한 이유입니다.\n",
        "    #기본적으로 batch가 아닌 데이터가 하나만 들어올떄도 코드가 \n",
        "    #동일하게 실행되도록 설계. \n",
        "    return dx\n",
        "    "
      ],
      "metadata": {
        "id": "kPKjs8XTT08M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pkwz5QUZXdfh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}