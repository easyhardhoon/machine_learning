{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcwyttbe+6Lr2Yp0BKo4fQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easyhardhoon/machine_learning/blob/master/ML6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKZ1m_hXm6Xv"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x0, x1 양쪽의 편미분을 묶어서 동시에 계산하기. 이때 (__, __) 처럼 모든 변수의 편미분을 벡터로 정리한 것을 기울기라고 함\n",
        "def numercial_gradient(f,x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x)\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]\n",
        "\n",
        "    x[idx] = tmp_val + h\n",
        "    fxh1 = f(x)\n",
        "\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x)\n",
        "\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "    x[idx] = tmp_val\n",
        "    # 변수가 하나일때의 수치 미분과 로직은 같음\n",
        "  return grad\n",
        "def function_2(x):\n",
        "  return x[0]**2 + x[1]**2"
      ],
      "metadata": {
        "id": "HHPem8t-m_o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numercial_gradient(function_2, np.array([3.0,4.0]))\n",
        "#각 점에서의 기울기. 벡터로 정리한 모양이다.  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-hXyEk0riVD",
        "outputId": "f98075af-c3bf-4341-9c3a-e4bfcae1d0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6., 8.])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**기울기가 의미하는 것**\n",
        "1. 일반적으로 기울기는 가장 낮은 장소르 가리킴. \n",
        "2. 반드시 기울기는 각 지점에서 낮아지는 방향을 가리킴\n",
        "---> 기울기가 가리키는 쪽은 각 장소에서 함수의 출력을 가장 크게 줄이는 방향이다\n",
        "이는 loss를 최대한 줄이는 최적의 알고리즘과 동일\n",
        "\n",
        "---> 이렇듯 기울기를 이용하여 함수의 최솟값을 찾으려는 방법이 경사법임\n",
        "\n",
        "기울기가 0인 부분을 찾으면 최적의 장소 (매개변수 수정 중지)를 찾은 것으로 간주.\n",
        "하지만 기울이가 0인 부분이 항상 최솟값은 아니다 \n",
        "\n",
        "ex) 극솟값 이거나 안장점 . 또는 끝없는 평지(고원)에  빠질 수 있음\n",
        "\n",
        "그래도 기울어진 방향이 꼭 최솟값을 가리키는 것은 아니지만 그 방향으로 가야 함수의 값을 줄일 수 있음.\n",
        "\n",
        "그래서 기울기 정보를 단서로 나아갈 방향을 정해야 함"
      ],
      "metadata": {
        "id": "w9Lw9yq0s90a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 경사법 : 현 위치에서 기울어진 방향으로 일정 거리만큼 이동. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고\n",
        "# 또 기울어진 방향으로 나아감. 이렇게해서 함수의 값을 점차 줄이는 것이 경사법. 마찬가지로 기울기를 이용함. \n",
        "# 학습률 : 갱신하는 양. 한번의 학습으로 얼마만큼 학습해야 할지. 매개변수 값을 얼마나 갱신해야할지\n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num = 100): #경사하강법 함수\n",
        "  x = init_x\n",
        "  for i in range(step_num):\n",
        "    grad = numercial_gradient(f,x)\n",
        "    x -= lr* grad\n",
        "  return x\n",
        "  #100번 x값 갱신. 기울기를 구한다음 학습률 값을 곱해 최대한 기울기가 0인 곳으로 도달한다. 극솟값을 구할 수 있다. 잘하면 최솟값도..!"
      ],
      "metadata": {
        "id": "ltGZeo0mtPuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def function_2(x):\n",
        "  return x[0]**2 + x[1]**2\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "#gradient_descent(function_2,init_x)\n",
        "gradient_descent(function_2, init_x, 0.1, 100)\n",
        "# 학습률 0.1, 100번 반복하여 기울기 기반 기울기가 0인 곳으로 최대한 도달한 결과값. 극솟값. 혹은 최솟값\n",
        "# 실제 최솟값은 (0,0)임 . 결과와 거의 일치하는 모습을 보인다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFpVpAlZMHhf",
        "outputId": "31d7805a-0d15-42e2-f5a4-89d25777afb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.11110793e-10,  8.14814391e-10])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 코드를 가시화하기 위해 복사해온 코드입니다.\n",
        "def numerical_gradient(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_no_batch(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
        "        \n",
        "        return grad\n",
        "def _numerical_gradient_no_batch(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        \n",
        "        # f(x+h) 계산\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)\n",
        "        \n",
        "        # f(x-h) 계산\n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x) \n",
        "        \n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "        \n",
        "    return grad\n",
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "#from gradient_2d import numerical_gradient\n",
        "\n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x = init_x\n",
        "    x_history = []\n",
        "\n",
        "    for i in range(step_num):\n",
        "        x_history.append( x.copy() )\n",
        "\n",
        "        grad = numerical_gradient(f, x)\n",
        "        x -= lr * grad\n",
        "\n",
        "    return x, np.array(x_history)\n",
        "\n",
        "\n",
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])    \n",
        "\n",
        "lr = 0.1\n",
        "step_num = 20\n",
        "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
        "\n",
        "plt.plot( [-5, 5], [0,0], '--b')\n",
        "plt.plot( [0,0], [-5, 5], '--b')\n",
        "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
        "\n",
        "plt.xlim(-3.5, 3.5)\n",
        "plt.ylim(-4.5, 4.5)\n",
        "plt.xlabel(\"X0\")\n",
        "plt.ylabel(\"X1\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "sKiY643gQQAM",
        "outputId": "c51d44c1-fa0f-459b-ba6f-31c63eb86773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이렇게 경사법을 이용해 최솟값을 탐색함.\n",
        "가장 값이 낮은 장소인 원점을 향해 가까워지고 있는 모습.\n",
        "함수의 최솟값 탐색인데, 이떄 함수로 loss함수를 사용하는듯. \n",
        "\n",
        "흔한 loss함수에는 오차제곱합, 교차 엔트로피 오차가 있다."
      ],
      "metadata": {
        "id": "vru_ZKh9UtdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습률\n",
        "학습률 값은 0.01 이나 0.001 등 미리 정해두어야 함\n",
        "이 값이 너무 크거나 작으면 좋은 장소를 찾아갈 수 없음.\n",
        "\n",
        "신경망 학습에서는 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행함"
      ],
      "metadata": {
        "id": "1sBylVSSV6FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(f, init_x, lr=0.01, step_num = 100): #경사하강법 함수\n",
        "  x = init_x\n",
        "  for i in range(step_num):\n",
        "    grad = numercial_gradient(f,x)\n",
        "    x -= lr* grad\n",
        "  return x\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "print(gradient_descent(function_2, init_x, 10.0, 100))\n",
        "#학습률이 너무 크면 큰 값으로 발산\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "print(gradient_descent(function_2, init_x, 1e-10, 100))\n",
        "#학습률이 너무 작으면 거의 갱신이 안되는 모습\n",
        "# 0,0 이 이상적 최솟값임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Ntu1hgWJF0",
        "outputId": "b3f173e4-f4d0-4e59-834a-0316ca7e91b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.58983747e+13 -1.29524862e+12]\n",
            "[-2.99999994  3.99999992]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OTK1XQRBaNeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼 파라미터 : 사람이 직접 설정  ex) 학습률 같은 매개변수\n",
        "\n",
        "일반 파라미터 : 훈련 데이터와 알고리즘으로 자동 획득 ex) 가중치, 편향 같은 신경망의 매개변수\n",
        "\n",
        "하이퍼 파라미터들은 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거친다"
      ],
      "metadata": {
        "id": "CkD7a5eTbgT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#신경망 학습에서의 기울기 : 가중치 매개변수에 대한 손실 함수의 기울기. 미분 하기 전 W와 미분 한 후 값인 dL/dw 의 형상은 같다\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from functions import softmax, cross_entropy_error\n",
        "from gradient import numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2,3) # 표준정규분포로 부터 난수 추출. 2 * 3 형상으로 \n",
        "  def predict(self,x):\n",
        "    return np.dot(x,self.W)\n",
        "  def loss(self, x, t): # x는 입력 데이터, t는 정답 레이블\n",
        "    z= self.predict(x)\n",
        "    y = softmax(z) # 활성화함수까지 돌린 결과값\n",
        "    loss = cross_entropy_error(y,t) #이 값 토대로 loss 값 계산. loss 함수로 교차엔트로피오차를 쓴 모습\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "8KhEwG74boSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = simpleNet()\n",
        "net.W #가중치 매개변수 2* 3 shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzMHG3wMyR8E",
        "outputId": "30c8cd30-77da-4f51-dd6f-024fa7a03514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.56788856,  0.00751605, -0.20807839],\n",
              "       [-0.56301546, -0.53786102, -0.07215984]])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([0.6,0.9])\n",
        "p = net.predict(x)\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jjryDIZ1tx_",
        "outputId": "40d4fa66-cb0b-4b33-d7fd-0e21b6b212f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.43401922, -0.47956529, -0.18979089])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDIRuAar16jp",
        "outputId": "d758f893-a121-44f0-827d-3173dee951a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.array([0,0,1]) # 복습. 입력 인수 ( 1 * 2), 가중치 매개변수 ( 2 * 3 ) ---> 결과예측값, 정답레이블 ( 1 * 3) 행렬곱 형상의 원칙\n",
        "net.loss(x,t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zou_AHhW2V1D",
        "outputId": "4a5da13c-733b-4a55-842e-e454430cf456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2849411845493006"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이제 편미분을 통해 기울기를 구해보기 .  \n",
        "# numerical_gradient의 인수 f, x 에 주목. 사실상 x는 f의 인수이다. 내부 구현에서 f(x)이렇게 실행이 됌\n",
        "def f(W):\n",
        "  return net.loss(x,t)  #  f = lambda W : net.loss(x,t) 로 간단히 쓸 수 있음\n",
        "dW = numerical_gradient(f, net.W) \n",
        "# 아까의 가설이 맞음. 결국 편미분 대상 함수는 loss함수이다. 결국 가중치 매개변수에 대한 손실 함수의 기울기를 구하는게 목적\n",
        "dW\n",
        "# dW와 미분전 net.W의 형상은  같다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZt6BDNh2jSm",
        "outputId": "3541d07d-7d42-4bf2-887a-87325ee8c4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.30976002,  0.12423977, -0.43399979],\n",
              "       [ 0.46464003,  0.18635966, -0.65099968]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#해석\n",
        "dW는 편미분의 결과. 2 * 3 배열.\n",
        "\n",
        "dw(11) 의 값은 대략 0.3 ---> w11을 h만큼 늘리면 함수(손실함수)는 0.3h만큼 증가한다. \n",
        "\n",
        "dw(23) 의 값은 대략 -0.6 ---> w23을 h만큼 늘리면 함수(손실함수)는 -0.6h만큼 감소한다.\n",
        " \n",
        "#손실함수를 줄인다는 관점 으로 보면 \n",
        "w23은 양의 방향으로 갱신해야하고 (잘하고 있으니)\n",
        "\n",
        "w11은 음의 방향으로 갱신해야함을 알 수 있다. (반대로 가야지)\n"
      ],
      "metadata": {
        "id": "kSdZd7GT9x0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IYXz9BKYCE_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재까지는 신경망의 기울기 값을 통해 함수(loss)의 최솟값 추정 좌표를 찾는 과정이었다. 이젠 이를 이용해 경사볍에 따라 가중치 매개변수 (W)의 갱신만 남았다.\n",
        "#사실\n",
        "신경망의 기울기는 가중치 매개변수에 대한 손실함수의 기울기이다. 즉 numercial_gradient(f,net.W)와 추후 gradient_descent(경사법)으로 loss함수에\n",
        "        대한 w매개변수로 해석할 수 있다. 즉 numbercial_gradient 결과값의 +-방향대로 w매개변수의 수정을 거치는게 바로 경사법에 따른 가중치 매개변수의 갱신이다\"\n",
        "\n",
        "쉽게 말해 이론적인 편미분 함수의 인자에 손실함수와 가중치매개변수를 넣는거임.\n",
        "\n",
        "x 좌표의 갱신이 아니고 weight 가중치 매개변수의 갱신임\n"
      ],
      "metadata": {
        "id": "5G7EtTr4Bqqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6Jm_oS7WHW6F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}