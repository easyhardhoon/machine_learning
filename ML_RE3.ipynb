{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_RE3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEcFEf8jWwFUWwIgdo+7vt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easyhardhoon/machine_learning_basic/blob/master/ML_RE3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⏰ML_RE2의 원핫인코딩 여부에 따른 cross_entropy_error함수의 코드 변화 복습"
      ],
      "metadata": {
        "id": "_qCnGL3i-OO2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvxFWoUu-KDW"
      },
      "outputs": [],
      "source": [
        "#원핫인코딩 false\n",
        "def cross_entropy_error(y,t):\n",
        "  if y.ndim == 1: \n",
        "    t = t.reshape(1,t.size)\n",
        "    y = y.reshape(1,y.size)\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "#원핫인코딩 true\n",
        "def cross_entropy_error(y,t): #배치를 고려하진 않았지만 로직은 같다\n",
        "  delta = 1e-7\n",
        "  return -np.sum(t*np.log(y+delta))\n",
        "\n",
        "#교차 엔트로피 오차는 원핫인코딩 여부와 관계없이 \"정답일때의\" 출력으로만 이루어져있다"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ML6~ML7"
      ],
      "metadata": {
        "id": "OMgl8zpB-tkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. np.zeros VS np.zeros_like**"
      ],
      "metadata": {
        "id": "8KYLd14mHrAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.zeros([2,3,3]) #인자(변수)의 shape만큼의 array 리턴\n",
        "print(a,\"\\n\")\n",
        "print(\"b의 형태는\")\n",
        "b = np.zeros_like([2,3,3]) #인자(변수)의 size만큼의 array 리턴\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAztbEe3-s3g",
        "outputId": "51fa0175-2fa0-4013-dbcd-3cb552c29041"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]] \n",
            "\n",
            "b의 형태는\n",
            "[0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 기울기의 의미**\n",
        "\n",
        "<하강법 기준>\n",
        "\n",
        "반드시 기울기는 각 지점에서 낮아지는 방향을 가리킨다.\n",
        "\n",
        "기울기가 0인 부분을 찾으면 최적의 장소를 찾은 것으로 간주하지만,\n",
        "\n",
        "극솟값이나 안장점의 예로 알 수 있듯 항상 기울기가 0인 부분이 최솟값은 아니다\n",
        "\n",
        "기울어진 방향이 꼭 최솟값을 가리키는 것은 아니지만 \n",
        "\n",
        "**기울기가 가리키는 방향으로 가야 함수(loss)의 값을 줄일 수 있다.**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "이 논리에 맞추어 기울기 정보를 토대로 가중치 매개변수의 값을 업데이트하고, \n",
        "\n",
        "(다음 루프에) 이 값을 토대로 loss 값을 줄여나간다\n",
        "\n",
        "\n",
        "---\n",
        "가중치 매개변수 업데이트 방식이 SGD, momentum, Adagrad,Adam..이든 관계없이\n",
        "\n",
        "기본적으로 기울기 정보(loss.backward()를 통해 얻어지는)를 사용한다\n",
        "\n",
        "---\n",
        "\n",
        "numerical_gradient(편미분) 또는 gradient(역전파)의 방법으로\n",
        "\n",
        "기울기 정보를 얻는다.(loss.backward()를 통해)\n",
        "\n",
        "편미분이든 역전파이든, 전체 학습 알고리즘은\n",
        "\n",
        "⭕**가중치 매개변수(parameter)에 대한 손실함수의 기울기 값을 토대로 parameter를 최적화 시키는 것이다**\n",
        "\n",
        "⭕**결국 손실함수(loss)의 기울기를 줄이는 쪽으로 학습이 쭉 이어진다**\n",
        "\n",
        "⭕**손실함수(loss)는 \"예측결과값\"과 \"정답 레이블\"의 비교값의 loss를 구하는 함수이다**\n",
        "\n",
        "⭕**이렇듯 예측값과 정답값의 오차를 줄이는 쪽으로 이어지는 것이 신경망 학습의 흐름이다**\n",
        "\n",
        "ex) numerical_gradient(f,w)  --> 이렇게 loss함수와 w 파라미터가 직접적으로 인자로 들어감\n",
        "\n",
        "ex) gradient(x,t) --> 사실 역전파는 살짝 다름. \n",
        "\n",
        "gradient함수 안에서 forward 와 backward 방향으로 흐르게 한 뒤\n",
        "\n",
        "(backward를 위해서 loss를 구해야함. 그래서 인자로 입력 데이터와 정답 레이블이 들어감)\n",
        "\n",
        "구해져있는 각 매개변수의 기울기 값(loss함수에 대한) 을 구해준다\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3NvAPByaJUzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚡지도학습의 알고리즘의 흐름\n",
        "\n",
        "1. 데이터 불러오기 & 전처리 & 배치\n",
        "\n",
        "2. optimizer(parameter 갱신방법), criterion(loss함수 구하는방법) 설정\n",
        "\n",
        "3. Net 구현. 또는 불러오기\n",
        "\n",
        "4. forward 수행\n",
        "\n",
        "5. criterion로 [forward로 구한 결과값(ouput)과 정답 레이블] 비교. \n",
        "\n",
        "6. 만들어진 단 하나의 스칼라 loss 기준의 backward 계산 (기울기 구하기)\n",
        "\n",
        "7. backward로 구한 기울기로  optimizer를 통한  parameter 업데이트\n",
        "\n",
        "8. (결과 출력을 위한) loss 값 누적 & 결과 산출\n",
        "\n",
        "9. 반복 (입력 데이터의 각 배치가 모두 끝날때까지)\n",
        "\n",
        "10. 반복을 반복 ( epoch. 전체 데이터를 몇 번 반복해서 볼 것인가)"
      ],
      "metadata": {
        "id": "ZdOkHty1SF-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 파라미터의 종류**\n",
        "\n",
        "▶하이퍼 파라미터 : 사람이 직접 설정\n",
        "\n",
        "ex) 학습률 같은 매개변수\n",
        "\n",
        "▶일반 파라미터 : 훈련 데이터와 알고리즘으로 자동 획득\n",
        "\n",
        "ex) 가중치, 편향 같은 신경망의 매개변수"
      ],
      "metadata": {
        "id": "IacFtreVysVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**학습률은**\n",
        "\n",
        "0.01이나 0.001같은 값을 많이 씁니다.\n",
        "\n",
        "너무 크면 큰 값으로 발산하고\n",
        "\n",
        "너무 작으면 거의 갱신이 되지 않습니다\n",
        "\n",
        "이러한 하이퍼 파라미터들은 여러 후보 값 중에서 \n",
        "\n",
        "시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거칩니다\n",
        "\n",
        "(ML13에서 다룹니다)"
      ],
      "metadata": {
        "id": "F2sw7M_Ly8-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. numercial_gradient(self,x,t)**"
      ],
      "metadata": {
        "id": "HFjbCFdc95jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W : self.loss(x,t)\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1']) \n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "    return grads"
      ],
      "metadata": {
        "id": "jDl25K9LH4Oi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 코드에서 보듯 numercial_gradient의 인자에 \n",
        "\n",
        "입려 데이터와 정답 레이블이 들어감을 볼 수 있다.\n",
        "\n",
        "기본적으로 numercial_gradient의 편미분을 통한 기울기 구하기 흐름은,\n",
        "\n",
        "가중치 매개변수에 대한 loss함수의 기울기 값을 구하는 것이다.\n",
        "\n",
        "즉, loss가 선행되어 구해져야 각 parameter에 대한 loss의 미분값을 구할 수 있다.\n",
        "\n",
        "그래서 최초의 numercial_gradient에 입력 데이터와 정답 레이블이 들어감을 알 수 있다."
      ],
      "metadata": {
        "id": "eZy4XCaf8EJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. np.random.choice**\n",
        "\n",
        "numpy.random.choice(a,size=None,replace=True,p=None)\n",
        "\n",
        "a : 1차원 배열 또는 정수\n",
        "\n",
        "size : 정수 또는 튜플\n",
        "\n",
        "replace : 중복 허용 여부\n",
        "\n",
        "p : 각 데이터가 선택될 확률"
      ],
      "metadata": {
        "id": "iTk9z5UdAqB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = np.random.choice(5,3,True)\n",
        "a\n",
        "# 0 이상 5미만인 정수 중 3개를 중복 허용하고 출력한다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsUOl7A18pYZ",
        "outputId": "f23048ad-f887-42d2-d554-efc01b2e974a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. batch**\n",
        "\n",
        "이제 입력 데이터가 한개씩이 아닌 batch형태로 들어간다. \n",
        "\n",
        "이에 맞게 비교를 위한 정답 레이블로 batch형태로 들어간다.\n",
        "\n",
        "앞의 원핫인코딩의 경우와 마찬가지로, 전체 학습 알고리즘은 변하지 않는다.\n",
        "\n",
        "다만 내부 함수 ( ex. gradient(x_batch,t_batch))의 구현이 조금 추가될 뿐이다\n",
        "\n",
        "(ex. if x.ndim == 1 : ...axis=1..)\n",
        "\n",
        "사실 나중가면 이렇게 입력 데이터가 1개이든 batch 이든 **통일된 코드**가 완성된다"
      ],
      "metadata": {
        "id": "IWF4EcAeDVq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. epoch**\n",
        "\n",
        "똑같은 학습과정을 몇번 반복하는가.\n",
        "\n",
        "예를 들어 전체 데이터가 100개이고 배치크기가 10이다.\n",
        "\n",
        " 이럴 경우 배치를 10번 돌려야 전체 데이터 한바퀴를 돌린 것이다. \n",
        "\n",
        " 이 한바퀴가 에폭이며, 이 전체 데이터를 10번 반복하여 돌린다면 에폭은 10이다"
      ],
      "metadata": {
        "id": "_yvdewx1G_G_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⏰예제 코드"
      ],
      "metadata": {
        "id": "EjfLJFDgI1oU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from mnist import load_mnist\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label=True)\n",
        "\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = [] # !!\n",
        "test_acc_list = [] # !!\n",
        "\n",
        "iters_num = 10000 \n",
        "train_size = x_train.shape[0] \n",
        "batch_size = 100 \n",
        "learning_rate = 0.1 \n",
        "network = TwoLayerNet(784, 50,10)\n",
        "\n",
        "iter_per_epoch = max(train_size/batch_size , 1)  \n",
        "\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  \n",
        "  #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "  grad = network.gradient(x_batch, t_batch) \n",
        "\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  if i % iter_per_epoch == 0: \n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(\"train acc, test acc :  \" + str(train_acc) + \", \" + str(test_acc))"
      ],
      "metadata": {
        "id": "tKCIiRA3BX_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**위 코드에서(위 코드는 입문용 단순화 코드임)**\n",
        "\n",
        "gradient를 사용하면 굳이... loss까지 안구하고 역전파를 구현할 수는 있는듯\n",
        "\n",
        "역전파의 개념대로 하면 출력값까지만 구하기만 하면 \n",
        "\n",
        "순전파의 반대방향으로 역전파를 흘려 보내기만 하면 되기 때문이다\n",
        "\n",
        "그래서 위 코드에서는 역전파를 먼저 돌려 parameter마다 기울기를 구한 뒤 loss를 구하는 모습\n",
        "\n",
        "---\n",
        "if i % iter_per_epoch == 0:\n",
        "\n",
        "의 의미는 그냥 \"에폭\"일때를 의미한다\n",
        "\n",
        "iter_per_epoch = 한 에폭당 반복수. "
      ],
      "metadata": {
        "id": "wdMHTHXvJJU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sEnPxRbmJPo9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}