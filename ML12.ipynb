{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2Y8Z4aETIkS2QFnB/3PBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easyhardhoon/machine_learning/blob/master/ML12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 가중치의 초깃값을 적절히 설정하여 활성화값이 잘 퍼지도록 하여 학습을 원활하게 하였다.\n",
        "\n",
        "그렇다면 각 층이 활성화를 적당히 퍼트리도록 강제해보면 어떨까\n",
        "\n",
        "이 알고리즘이 **배치 정규화**이다"
      ],
      "metadata": {
        "id": "b1XQCr82o1Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#배치 정규화의 아이디어\n",
        "\n",
        "각 층에서의 활성화값이 적당히 분포되도록 조정한다. \n",
        "\n",
        "데이터 분포를 정규화하는 **배치 정규화 계층**을 신경망에 삽입한다\n",
        "\n",
        "Affine -> batch norm -> Relu(활성화 함수)  이런 식으로!!\n",
        "\n",
        "단순히 미니배치 입력 데이터들 (원래는 이들 고유의 평균 m, 분산 o가 있을 것이다)을 평균 0, 분산 1인 데이터로 변환(정규화)한다. \n",
        "\n",
        "ex) **평균m.분산e 입력 데이터** {x1,x2,....xm} ----> **평균0,분산1 변환 데이터** {x^1,x^2....x^m}\n",
        "\n",
        "이러한 처리를 활성화 함수의 앞 혹은 뒤에 삽입함으로써 데이터 분포가 덜 치우치게 할 수 있다.\n",
        "\n",
        "또한 배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대와 이동 변환을 수행한다.\n",
        "\n",
        "수식 : yi = R* x^i + B ( R : 확대, B : 이동 )\n",
        "\n",
        "두 값은 처음에는 (1,0) [원본그대로] 이었다가, 학습을 하면서 적합한 값으로 조정해나간다\n",
        "\n",
        "이 과정이 배치 정규화의 알고리즘이며, 신경망에서 순전파일때 적용된다.\n",
        "\n",
        "+ 역전파일때는? 다소 복잡하다. 근데..이러면 역전파의 흐름이 최신화되어야 하는거 아닌가..? \"배치 정규화\"의 layer가 추가되는것 아닌가..?\n",
        "\n",
        "+ 앞선 layer 계층의 알고리즘대로 구현하면 자연스럽게 \"배치 정규화\"의 계층도 추가되고 역전파의 알고리즘도 그대로 유지할 수 있을 것 같다....\n",
        "\n",
        "#배치 정규화의 효과\n",
        "\n",
        "거의 모든 경우에서 배치 정규화를 사용할 때의 학습 진도가 빠르다.\n",
        "\n",
        "( 오히려 초깃값의 표준편차가 적절하면 배치 정규화를 안하는게 더 나을떄도 있다)\n",
        "\n",
        "\n",
        "실제로 배치 정규화를 이용하지 않는 경우엔 초깃값이 잘 분포되지 않으면 학습이 전혀 진행되지 않는 경우도 있다 ( ML11 참고)\n",
        "\n",
        "#결론\n",
        "\n",
        "배치 정규화를 사용하면 학습이 빨라지며, 가중치 초깃값에 크게 의존하지 않아도 된다. 이러한 부분때문에 다양한 분야에서 활약한다"
      ],
      "metadata": {
        "id": "X3KXv7QcpwmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#오버피팅\n",
        "\n",
        "오버피팅 : 신경망이 훈련 데이터에만 지나치게 적응되어 그 외의 데이터에는 제대로 적응하지 못하는 상태\n",
        "\n",
        "기계학습은 범용 성능을 지향하기 떄문에, 오버피팅을 억제하는 기술이 중요하다\n",
        "\n",
        "**오버피팅이 발생하는 경우**\n",
        "+ 매개변수가 많고 표현력이 높은 모델\n",
        "+ 훈련 데이터가 적음 \n",
        "\n"
      ],
      "metadata": {
        "id": "rkiHo3kQG00T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#아래 코드는 오버피팅을 일으켜보기 위한 코드입니다. "
      ],
      "metadata": {
        "id": "G9CrYwWpJUCq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "from functions import *\n",
        "from util import im2col, col2im\n",
        "\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "\n",
        "\n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var  \n",
        "        \n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "        \n",
        "        return out.reshape(*self.input_shape)\n",
        "            \n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "                        \n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = np.mean(xc**2, axis=0)\n",
        "            std = np.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "            \n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "            \n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = np.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "        \n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "        \n",
        "        return dx\n",
        "\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        # 중간 데이터（backward 시 사용）\n",
        "        self.x = None   \n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "        \n",
        "        # 가중치와 편향 매개변수의 기울기\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        self.dW = np.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = np.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "#from layers import *\n",
        "from gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class MultiLayerNet:\n",
        "    \"\"\"완전연결 다층 신경망\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size_list, output_size,\n",
        "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.weight_decay_lambda = weight_decay_lambda\n",
        "        self.params = {}\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.__init_weight(weight_init_std)\n",
        "\n",
        "        # 계층 생성\n",
        "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
        "        self.layers = OrderedDict()\n",
        "        for idx in range(1, self.hidden_layer_num+1):\n",
        "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
        "                                                      self.params['b' + str(idx)])\n",
        "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
        "\n",
        "        idx = self.hidden_layer_num + 1\n",
        "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
        "            self.params['b' + str(idx)])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def __init_weight(self, weight_init_std):\n",
        "        \"\"\"가중치 초기화\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "        \"\"\"\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값\n",
        "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
        "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블 \n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        손실 함수의 값\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "\n",
        "        weight_decay = 0\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            W = self.params['W' + str(idx)]\n",
        "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
        "\n",
        "        return self.last_layer.forward(y, t) + weight_decay\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(수치 미분).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num+2):\n",
        "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n",
        "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "GoZSm-xtJAex"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a8qkIK90oyS1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a6c1f7d-41d9-47a5-8d68-6982473300e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, train acc:0.10333333333333333, test acc:0.1087\n",
            "epoch:1, train acc:0.14666666666666667, test acc:0.1328\n",
            "epoch:2, train acc:0.19, test acc:0.1591\n",
            "epoch:3, train acc:0.20666666666666667, test acc:0.1855\n",
            "epoch:4, train acc:0.27, test acc:0.2146\n",
            "epoch:5, train acc:0.30666666666666664, test acc:0.2354\n",
            "epoch:6, train acc:0.34, test acc:0.2605\n",
            "epoch:7, train acc:0.36, test acc:0.2825\n",
            "epoch:8, train acc:0.36666666666666664, test acc:0.2909\n",
            "epoch:9, train acc:0.38666666666666666, test acc:0.3005\n",
            "epoch:10, train acc:0.39, test acc:0.3098\n",
            "epoch:11, train acc:0.4166666666666667, test acc:0.33\n",
            "epoch:12, train acc:0.4533333333333333, test acc:0.3458\n",
            "epoch:13, train acc:0.4666666666666667, test acc:0.357\n",
            "epoch:14, train acc:0.4866666666666667, test acc:0.3659\n",
            "epoch:15, train acc:0.52, test acc:0.3818\n",
            "epoch:16, train acc:0.5366666666666666, test acc:0.3958\n",
            "epoch:17, train acc:0.5433333333333333, test acc:0.4088\n",
            "epoch:18, train acc:0.5633333333333334, test acc:0.4195\n",
            "epoch:19, train acc:0.57, test acc:0.4325\n",
            "epoch:20, train acc:0.58, test acc:0.4439\n",
            "epoch:21, train acc:0.5833333333333334, test acc:0.4552\n",
            "epoch:22, train acc:0.6033333333333334, test acc:0.4643\n",
            "epoch:23, train acc:0.6133333333333333, test acc:0.4733\n",
            "epoch:24, train acc:0.6233333333333333, test acc:0.4777\n",
            "epoch:25, train acc:0.6266666666666667, test acc:0.4752\n",
            "epoch:26, train acc:0.6466666666666666, test acc:0.4912\n",
            "epoch:27, train acc:0.6433333333333333, test acc:0.4994\n",
            "epoch:28, train acc:0.66, test acc:0.5064\n",
            "epoch:29, train acc:0.65, test acc:0.5118\n",
            "epoch:30, train acc:0.6566666666666666, test acc:0.5062\n",
            "epoch:31, train acc:0.68, test acc:0.5175\n",
            "epoch:32, train acc:0.6666666666666666, test acc:0.5157\n",
            "epoch:33, train acc:0.6833333333333333, test acc:0.5242\n",
            "epoch:34, train acc:0.67, test acc:0.5288\n",
            "epoch:35, train acc:0.6766666666666666, test acc:0.5183\n",
            "epoch:36, train acc:0.6833333333333333, test acc:0.5245\n",
            "epoch:37, train acc:0.6933333333333334, test acc:0.5267\n",
            "epoch:38, train acc:0.7033333333333334, test acc:0.5366\n",
            "epoch:39, train acc:0.7433333333333333, test acc:0.5553\n",
            "epoch:40, train acc:0.76, test acc:0.5658\n",
            "epoch:41, train acc:0.7333333333333333, test acc:0.5618\n",
            "epoch:42, train acc:0.7533333333333333, test acc:0.5738\n",
            "epoch:43, train acc:0.7833333333333333, test acc:0.5927\n",
            "epoch:44, train acc:0.7866666666666666, test acc:0.6018\n",
            "epoch:45, train acc:0.7833333333333333, test acc:0.603\n",
            "epoch:46, train acc:0.7866666666666666, test acc:0.6097\n",
            "epoch:47, train acc:0.7633333333333333, test acc:0.5841\n",
            "epoch:48, train acc:0.7633333333333333, test acc:0.602\n",
            "epoch:49, train acc:0.7733333333333333, test acc:0.6041\n",
            "epoch:50, train acc:0.78, test acc:0.6063\n",
            "epoch:51, train acc:0.7666666666666667, test acc:0.6056\n",
            "epoch:52, train acc:0.79, test acc:0.6171\n",
            "epoch:53, train acc:0.8033333333333333, test acc:0.6246\n",
            "epoch:54, train acc:0.8, test acc:0.6257\n",
            "epoch:55, train acc:0.7933333333333333, test acc:0.6264\n",
            "epoch:56, train acc:0.8066666666666666, test acc:0.6374\n",
            "epoch:57, train acc:0.8166666666666667, test acc:0.6441\n",
            "epoch:58, train acc:0.8133333333333334, test acc:0.644\n",
            "epoch:59, train acc:0.8166666666666667, test acc:0.6407\n",
            "epoch:60, train acc:0.83, test acc:0.641\n",
            "epoch:61, train acc:0.8166666666666667, test acc:0.649\n",
            "epoch:62, train acc:0.83, test acc:0.6501\n",
            "epoch:63, train acc:0.83, test acc:0.6564\n",
            "epoch:64, train acc:0.8333333333333334, test acc:0.66\n",
            "epoch:65, train acc:0.81, test acc:0.6451\n",
            "epoch:66, train acc:0.8366666666666667, test acc:0.6543\n",
            "epoch:67, train acc:0.8433333333333334, test acc:0.6658\n",
            "epoch:68, train acc:0.8366666666666667, test acc:0.6612\n",
            "epoch:69, train acc:0.8266666666666667, test acc:0.665\n",
            "epoch:70, train acc:0.8566666666666667, test acc:0.6683\n",
            "epoch:71, train acc:0.8533333333333334, test acc:0.6664\n",
            "epoch:72, train acc:0.8533333333333334, test acc:0.6697\n",
            "epoch:73, train acc:0.84, test acc:0.6578\n",
            "epoch:74, train acc:0.84, test acc:0.6632\n",
            "epoch:75, train acc:0.8566666666666667, test acc:0.6777\n",
            "epoch:76, train acc:0.8466666666666667, test acc:0.6738\n",
            "epoch:77, train acc:0.8533333333333334, test acc:0.6792\n",
            "epoch:78, train acc:0.8666666666666667, test acc:0.6835\n",
            "epoch:79, train acc:0.8733333333333333, test acc:0.6847\n",
            "epoch:80, train acc:0.8733333333333333, test acc:0.6833\n",
            "epoch:81, train acc:0.86, test acc:0.6724\n",
            "epoch:82, train acc:0.8766666666666667, test acc:0.6855\n",
            "epoch:83, train acc:0.8633333333333333, test acc:0.6754\n",
            "epoch:84, train acc:0.8533333333333334, test acc:0.6741\n",
            "epoch:85, train acc:0.8733333333333333, test acc:0.6796\n",
            "epoch:86, train acc:0.86, test acc:0.6762\n",
            "epoch:87, train acc:0.8666666666666667, test acc:0.6745\n",
            "epoch:88, train acc:0.8633333333333333, test acc:0.6763\n",
            "epoch:89, train acc:0.8733333333333333, test acc:0.6817\n",
            "epoch:90, train acc:0.8733333333333333, test acc:0.6843\n",
            "epoch:91, train acc:0.8733333333333333, test acc:0.6801\n",
            "epoch:92, train acc:0.8666666666666667, test acc:0.6739\n",
            "epoch:93, train acc:0.8633333333333333, test acc:0.6825\n",
            "epoch:94, train acc:0.88, test acc:0.6874\n",
            "epoch:95, train acc:0.8866666666666667, test acc:0.6822\n",
            "epoch:96, train acc:0.8833333333333333, test acc:0.6826\n",
            "epoch:97, train acc:0.8866666666666667, test acc:0.6888\n",
            "epoch:98, train acc:0.8933333333333333, test acc:0.6976\n",
            "epoch:99, train acc:0.8866666666666667, test acc:0.6886\n",
            "epoch:100, train acc:0.8933333333333333, test acc:0.6953\n",
            "epoch:101, train acc:0.8933333333333333, test acc:0.7011\n",
            "epoch:102, train acc:0.8866666666666667, test acc:0.6958\n",
            "epoch:103, train acc:0.9033333333333333, test acc:0.6989\n",
            "epoch:104, train acc:0.8933333333333333, test acc:0.6857\n",
            "epoch:105, train acc:0.8866666666666667, test acc:0.6855\n",
            "epoch:106, train acc:0.89, test acc:0.6941\n",
            "epoch:107, train acc:0.88, test acc:0.6928\n",
            "epoch:108, train acc:0.8833333333333333, test acc:0.6963\n",
            "epoch:109, train acc:0.8833333333333333, test acc:0.6937\n",
            "epoch:110, train acc:0.89, test acc:0.6899\n",
            "epoch:111, train acc:0.8966666666666666, test acc:0.6991\n",
            "epoch:112, train acc:0.89, test acc:0.6923\n",
            "epoch:113, train acc:0.8833333333333333, test acc:0.6954\n",
            "epoch:114, train acc:0.8966666666666666, test acc:0.6957\n",
            "epoch:115, train acc:0.91, test acc:0.7016\n",
            "epoch:116, train acc:0.8933333333333333, test acc:0.6857\n",
            "epoch:117, train acc:0.8966666666666666, test acc:0.6988\n",
            "epoch:118, train acc:0.8966666666666666, test acc:0.6991\n",
            "epoch:119, train acc:0.9166666666666666, test acc:0.7038\n",
            "epoch:120, train acc:0.9166666666666666, test acc:0.7046\n",
            "epoch:121, train acc:0.9, test acc:0.7049\n",
            "epoch:122, train acc:0.9, test acc:0.7056\n",
            "epoch:123, train acc:0.91, test acc:0.7047\n",
            "epoch:124, train acc:0.9066666666666666, test acc:0.7086\n",
            "epoch:125, train acc:0.91, test acc:0.7074\n",
            "epoch:126, train acc:0.92, test acc:0.7072\n",
            "epoch:127, train acc:0.8966666666666666, test acc:0.7082\n",
            "epoch:128, train acc:0.9066666666666666, test acc:0.7077\n",
            "epoch:129, train acc:0.91, test acc:0.708\n",
            "epoch:130, train acc:0.89, test acc:0.7048\n",
            "epoch:131, train acc:0.8966666666666666, test acc:0.705\n",
            "epoch:132, train acc:0.9, test acc:0.705\n",
            "epoch:133, train acc:0.89, test acc:0.7061\n",
            "epoch:134, train acc:0.91, test acc:0.713\n",
            "epoch:135, train acc:0.9133333333333333, test acc:0.7107\n",
            "epoch:136, train acc:0.9033333333333333, test acc:0.7159\n",
            "epoch:137, train acc:0.9, test acc:0.7168\n",
            "epoch:138, train acc:0.9166666666666666, test acc:0.7153\n",
            "epoch:139, train acc:0.9, test acc:0.7134\n",
            "epoch:140, train acc:0.9133333333333333, test acc:0.7054\n",
            "epoch:141, train acc:0.91, test acc:0.7058\n",
            "epoch:142, train acc:0.9066666666666666, test acc:0.705\n",
            "epoch:143, train acc:0.9066666666666666, test acc:0.7068\n",
            "epoch:144, train acc:0.9, test acc:0.7057\n",
            "epoch:145, train acc:0.9, test acc:0.7094\n",
            "epoch:146, train acc:0.9166666666666666, test acc:0.7145\n",
            "epoch:147, train acc:0.91, test acc:0.711\n",
            "epoch:148, train acc:0.9166666666666666, test acc:0.7154\n",
            "epoch:149, train acc:0.9166666666666666, test acc:0.7137\n",
            "epoch:150, train acc:0.91, test acc:0.7032\n",
            "epoch:151, train acc:0.91, test acc:0.7032\n",
            "epoch:152, train acc:0.91, test acc:0.7088\n",
            "epoch:153, train acc:0.9133333333333333, test acc:0.7114\n",
            "epoch:154, train acc:0.91, test acc:0.71\n",
            "epoch:155, train acc:0.91, test acc:0.7045\n",
            "epoch:156, train acc:0.9166666666666666, test acc:0.7077\n",
            "epoch:157, train acc:0.9266666666666666, test acc:0.7078\n",
            "epoch:158, train acc:0.9066666666666666, test acc:0.7068\n",
            "epoch:159, train acc:0.9133333333333333, test acc:0.7111\n",
            "epoch:160, train acc:0.9166666666666666, test acc:0.7127\n",
            "epoch:161, train acc:0.92, test acc:0.7076\n",
            "epoch:162, train acc:0.92, test acc:0.7135\n",
            "epoch:163, train acc:0.9166666666666666, test acc:0.7088\n",
            "epoch:164, train acc:0.9266666666666666, test acc:0.7093\n",
            "epoch:165, train acc:0.92, test acc:0.7129\n",
            "epoch:166, train acc:0.92, test acc:0.7157\n",
            "epoch:167, train acc:0.9233333333333333, test acc:0.7129\n",
            "epoch:168, train acc:0.9166666666666666, test acc:0.716\n",
            "epoch:169, train acc:0.9166666666666666, test acc:0.7118\n",
            "epoch:170, train acc:0.9233333333333333, test acc:0.7134\n",
            "epoch:171, train acc:0.92, test acc:0.7118\n",
            "epoch:172, train acc:0.92, test acc:0.712\n",
            "epoch:173, train acc:0.9266666666666666, test acc:0.7171\n",
            "epoch:174, train acc:0.93, test acc:0.7146\n",
            "epoch:175, train acc:0.9133333333333333, test acc:0.7127\n",
            "epoch:176, train acc:0.92, test acc:0.7111\n",
            "epoch:177, train acc:0.92, test acc:0.7132\n",
            "epoch:178, train acc:0.9133333333333333, test acc:0.7075\n",
            "epoch:179, train acc:0.92, test acc:0.7131\n",
            "epoch:180, train acc:0.9166666666666666, test acc:0.7101\n",
            "epoch:181, train acc:0.9033333333333333, test acc:0.71\n",
            "epoch:182, train acc:0.9133333333333333, test acc:0.7135\n",
            "epoch:183, train acc:0.9233333333333333, test acc:0.718\n",
            "epoch:184, train acc:0.9233333333333333, test acc:0.7101\n",
            "epoch:185, train acc:0.9233333333333333, test acc:0.712\n",
            "epoch:186, train acc:0.92, test acc:0.7143\n",
            "epoch:187, train acc:0.9166666666666666, test acc:0.7083\n",
            "epoch:188, train acc:0.9166666666666666, test acc:0.7088\n",
            "epoch:189, train acc:0.9133333333333333, test acc:0.697\n",
            "epoch:190, train acc:0.9266666666666666, test acc:0.7067\n",
            "epoch:191, train acc:0.9266666666666666, test acc:0.7099\n",
            "epoch:192, train acc:0.9166666666666666, test acc:0.7052\n",
            "epoch:193, train acc:0.91, test acc:0.71\n",
            "epoch:194, train acc:0.9066666666666666, test acc:0.7056\n",
            "epoch:195, train acc:0.93, test acc:0.7116\n",
            "epoch:196, train acc:0.9166666666666666, test acc:0.7074\n",
            "epoch:197, train acc:0.9233333333333333, test acc:0.7126\n",
            "epoch:198, train acc:0.9266666666666666, test acc:0.7147\n",
            "epoch:199, train acc:0.9233333333333333, test acc:0.7136\n",
            "epoch:200, train acc:0.92, test acc:0.7166\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#여기서부터 진짜 코드입니다.\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "#from multi_layer_net import MultiLayerNet\n",
        "from optimizer import SGD\n",
        "\n",
        "(x_train,t_train), (x_test,t_test) = load_mnist(normalize=True)\n",
        "\n",
        "x_train = x_train[:300] #6만개의 데이터 셋중 300개만 사용한 모습입니다\n",
        "t_train = t_train[:300]\n",
        "\n",
        "weight_decay_lambda = 0.1\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list = [100,100,100,100,100,100], \\\n",
        "                        output_size =10, weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer= SGD(lr=0.01) # 학습률 0.01인 SGD로 매개변수를 갱신하겠습니다.\n",
        "\n",
        "max_epochs= 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0: # 지금까지의 코드와 같습니다. 에폭마다 모든 훈련데이터와 시험데이터의 정확도를 산출합니다. \n",
        "        # 최종적으로 두 데이터에 대한 정확도를 구하기 위한 과정일뿐 입니다.\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs: #일정한 범위를 넘어서면 break 해버립니다.\n",
        "            break\n",
        "\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#위 코드에서\n",
        "\n",
        "기본 MNIST의 데이터셋의 훈련 데이터 6만개중 300개만 사용하였고, 7층 네트워크를 사용하여 네트워크의 복잡성을 높였다.\n",
        "\n",
        "각 층의 뉴런은 100개로 설정하였다. \n",
        "\n",
        "즉, MNIST 데이터셋의 훈련데이터를 적게 하였고, 각 층의  뉴런을 100개로 설정함으로써 매개변수가 많은 모델을 선정하였다\n",
        "\n",
        "위 그래프에서, 훈련 데이터는 100 에폭을 지나는 무렵부터 거의 100프로이지만\n",
        "\n",
        "시험 데이터와는 점점 큰 차이를 보인다. 이렇게 정확도가 크게 벌어지는 것은 훈련 데이터에만 적응해버린 결과이다.\n",
        "\n",
        "훈련 떄 사용하지 않는 범용 데이터에 제대로 대응하지 못하는 모습을 볼 수 있다. 즉 오버피팅이 일어남을 볼 수 있다."
      ],
      "metadata": {
        "id": "JICoSREJOWEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MultiLayerNet 인자의 의미 \n",
        "\n",
        "TwoLayerNet(input_size, hidden_size, output_size) 에서 각각의 의미는 \n",
        "\n",
        "입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수 이었습니다. \n",
        "\n",
        "각각의 층의 뉴런 수는, 가중치 매개변수의 개수라고 생각해도 될 듯 합니다.\n",
        "\n",
        "그렇다면  MultiLayerNet(input_size, hidden_size_list, output_size) 에서 \n",
        "\n",
        "단순히 multilayer 이기 떄문에 hidden_size, 즉 각 층마다의 은닉층 뉴런수를 모아둔\n",
        "\n",
        "hidden_size_list가 필요함을 알 수 있습니다.\n",
        "\n",
        "즉, **hidden_size_list = [100,100,100,100,100,100] 의 뜻**은, **100개의 뉴런(매개변수)가 있는 은닉층이 6개 있다라는 소리로 해석할 수 있습니다.**\n",
        "\n",
        "따라서 전체 네트워크는 7층임을 알 수 있습니다. \n"
      ],
      "metadata": {
        "id": "jntKbq2lRn_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ML13부터는\n",
        "\n",
        "import를 간단히 구현하여 일일이 복잡한 코드를 복사하는 일이 없도록 하겠습니다.\n"
      ],
      "metadata": {
        "id": "QMsheOS4Ppx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "30F1dMN7I30b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}