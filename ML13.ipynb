{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxL6FTrvJRO2ARVIQz3j+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easyhardhoon/machine_learning/blob/master/ML13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#오버피팅을 억제하는 방법\n",
        "\n",
        "#1. 가중치 감소\n",
        "\n",
        "오버피팅은 가중치 매개변수의 값이 커서 발생하는 경우가 많다\n",
        "\n",
        "그래서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 오버피팅을 억제하는 방법이 가중치 감소이다\n",
        "\n",
        "**가중치가 W였으면 가중치 감소를 통해 1/2ㅅW^2로 바꾼다**. \n",
        "\n",
        "**이 값을 손실함수에 더한다.**\n",
        "\n",
        "가중치가 너무 크면 손실 함수의 값이 너무 확 줄어드니 그걸 방지하기 위한 것이라고 생각\n",
        "\n",
        "ㅅ(람다) : 정규화의 세기를 조절하는 하이퍼파라미터\n",
        "\n",
        "L2 norm : 각 원소의 제곱들을 더한 값\n",
        "\n",
        "#즉\n",
        "\n",
        "가중치 감소는 모든 가중치 각각의 손실 함수에 1/2ㅅW^2를 더한다\n",
        "\n",
        "가중치의 기울기를 구하는 계산에서, 오차역전법의 결과에 (이 정규화 항을 미분한) **ㅅW**을 더한다\n",
        "\n",
        "아래의 가중치 감소를 적용한 코드의 그래프를 보면,\n",
        "\n",
        "오버피팅이 억제된 모습을 볼 수 있다. + 훈련 데이터에 대한 정확도가 100%에 도달하지 못하였다\n",
        "\n"
      ],
      "metadata": {
        "id": "9XOUfpKB6pat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from multi_layer_net import MultiLayerNet\n",
        "from optimizer import SGD\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay（가중치 감쇠） 설정 =======================\n",
        "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
        "weight_decay_lambda = 0.1\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
        "                        weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        #print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "WSKnyE0vR2QB",
        "outputId": "df40369a-62b5-4b87-a07b-a2eb395633a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. 드롭아웃\n",
        "\n",
        "가중치 감소는 간단하게 구현할 수 있고 어느 정도 지나친 학습을 억제할 수 있지만\n",
        "\n",
        "신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어렵다.\n",
        "\n",
        "이럴때는 드롭아웃이라는 기법을 이용한다.\n",
        "\n",
        "드롭아웃은, 뉴런을 임의로 삭제하며 학습하는 방법이다.\n",
        "\n",
        "훈련떄 은닉층의 뉴런을 무작위로 골라 삭제한다. \n",
        "\n",
        "훈련때는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택하고, 시험 때는 모든 뉴런에 신호를 전달한다. \n",
        "\n",
        "단, 시험때는 각 뉴련의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력한다\n",
        "\n",
        "(사실 곱하지 않아도 좋다, 실제 딥러닝 프레임워크들도 비율을 곱하지 않음)\n",
        "\n"
      ],
      "metadata": {
        "id": "sEYnsBUwIly8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIUg2J5z5t_6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class Dropout:\n",
        "  def __init__(self,dropout_ratio = 0.5):\n",
        "    self.dropout_ratio = dropout_ratio\n",
        "    self.mask = None\n",
        "  def forward(self,x,train_flg=True):\n",
        "    if train_flg:\n",
        "      self.mask = np.random.rand(*x.shape) > self.dropout_ratio #ratio보다 큰 값만 true(1), 아니면 false(0)\n",
        "      #갑자기 *가 나와서 당황할 수 있겠지만\n",
        "      #x.shape의 형식이 튜플이기 때문에 구분하기 위하여 썼다고 생각.\n",
        "      #즉 x.shape가 (2.3)이면 np.random.rand(2,3)과 같은 말이라고 할 수 있다.\n",
        "      return x * self.mask #결론적으로 ratio기준 큰 값들만 올바르게 흐르고 나머지는 막아버린다는 느낌 구현 완료\n",
        "    else:\n",
        "      return x * (1.0 * self.dropout_ratio)\n",
        "  def backward(self,dout):\n",
        "    return dout * self.mask #순전파때 흘렀던건 역전파떄도 흐르게 한다. RELU 와 동일하다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import load_mnist\n",
        "from multi_layer_net_extend import MultiLayerNetExtend\n",
        "from trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
        "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(input_size=784,\n",
        "                              hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                              output_size=10, use_dropout=use_dropout,\n",
        "                              dropout_ration=dropout_ratio)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "# epoch:301, train acc:0.73, test acc:0.6315"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TE14ac0yKYya",
        "outputId": "7dce779a-4f4e-42a5-abc3-62578e17c70a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.3030886320811566\n",
            "train loss:2.313986877407042\n",
            "train loss:2.30606893076447\n",
            "train loss:2.3009966691834\n",
            "train loss:2.295038746575266\n",
            "train loss:2.2969212260932466\n",
            "train loss:2.30133550858684\n",
            "train loss:2.299074783370869\n",
            "train loss:2.2866692813120544\n",
            "train loss:2.2948186066597778\n",
            "train loss:2.3139748675176643\n",
            "train loss:2.299244586410641\n",
            "train loss:2.2946170315751426\n",
            "train loss:2.300189328083236\n",
            "train loss:2.297163349955228\n",
            "train loss:2.2885853050506113\n",
            "train loss:2.294458989568734\n",
            "train loss:2.297352174089609\n",
            "train loss:2.2928088390232197\n",
            "train loss:2.2908185572208386\n",
            "train loss:2.294403799298263\n",
            "train loss:2.2971640530776583\n",
            "train loss:2.2882670026050227\n",
            "train loss:2.3025196069252547\n",
            "train loss:2.2982791971014\n",
            "train loss:2.2877057267654552\n",
            "train loss:2.291462331936229\n",
            "train loss:2.2918112204760375\n",
            "train loss:2.289714964675606\n",
            "train loss:2.3008443936625955\n",
            "train loss:2.2839963575718447\n",
            "train loss:2.298601455495009\n",
            "train loss:2.293311690283478\n",
            "train loss:2.278609004580513\n",
            "train loss:2.286511240242684\n",
            "train loss:2.2866289512952367\n",
            "train loss:2.280790968466775\n",
            "train loss:2.2926809072387417\n",
            "train loss:2.272135240678574\n",
            "train loss:2.2732195467394383\n",
            "train loss:2.2796187750403885\n",
            "train loss:2.2758691786365843\n",
            "train loss:2.278248095299209\n",
            "train loss:2.286524785104083\n",
            "train loss:2.283161823944755\n",
            "train loss:2.282771628259057\n",
            "train loss:2.2904543738022802\n",
            "train loss:2.297213075564352\n",
            "train loss:2.2800989077570732\n",
            "train loss:2.2885882906797166\n",
            "train loss:2.2878367355007363\n",
            "train loss:2.2858636329886806\n",
            "train loss:2.278729274994069\n",
            "train loss:2.2802952104347805\n",
            "train loss:2.2818265363454855\n",
            "train loss:2.2910985723765664\n",
            "train loss:2.283693229823647\n",
            "train loss:2.2756308540395036\n",
            "train loss:2.274847039391471\n",
            "train loss:2.2808879628659757\n",
            "train loss:2.2869414346707\n",
            "train loss:2.2758093183020485\n",
            "train loss:2.2813103832070847\n",
            "train loss:2.264719377413167\n",
            "train loss:2.276795259049827\n",
            "train loss:2.2764359864199655\n",
            "train loss:2.2726996250733085\n",
            "train loss:2.2731256653929703\n",
            "train loss:2.277097894151258\n",
            "train loss:2.275734626885221\n",
            "train loss:2.2538028947701974\n",
            "train loss:2.2676770907767123\n",
            "train loss:2.276767087858659\n",
            "train loss:2.2732411573544358\n",
            "train loss:2.268732642735674\n",
            "train loss:2.2762776341208446\n",
            "train loss:2.2847234931070086\n",
            "train loss:2.2844190163458054\n",
            "train loss:2.274603159881267\n",
            "train loss:2.280051400969529\n",
            "train loss:2.2677996302422434\n",
            "train loss:2.2746648874582722\n",
            "train loss:2.266025760009103\n",
            "train loss:2.2724155384003923\n",
            "train loss:2.2646475277579783\n",
            "train loss:2.2712126970785733\n",
            "train loss:2.2608375013968764\n",
            "train loss:2.2756853727606274\n",
            "train loss:2.2686426483343625\n",
            "train loss:2.2728852427055126\n",
            "train loss:2.281364435625083\n",
            "train loss:2.2650855952219304\n",
            "train loss:2.272673080725985\n",
            "train loss:2.260967970117255\n",
            "train loss:2.2665472430559395\n",
            "train loss:2.2540997186811267\n",
            "train loss:2.2609583874093\n",
            "train loss:2.2653661016354762\n",
            "train loss:2.2657667930380243\n",
            "train loss:2.2834925713487304\n",
            "train loss:2.2649745179039296\n",
            "train loss:2.259022978664044\n",
            "train loss:2.255548337679586\n",
            "train loss:2.2643133164645226\n",
            "train loss:2.251889111258993\n",
            "train loss:2.2586262528299073\n",
            "train loss:2.267853969827051\n",
            "train loss:2.255974084729378\n",
            "train loss:2.268089569501814\n",
            "train loss:2.256289080441075\n",
            "train loss:2.2663757913047875\n",
            "train loss:2.25543500369268\n",
            "train loss:2.2597289216329086\n",
            "train loss:2.261190397500914\n",
            "train loss:2.267488905829097\n",
            "train loss:2.260278410640434\n",
            "train loss:2.266900125780573\n",
            "train loss:2.2630387920681487\n",
            "train loss:2.25924971721556\n",
            "train loss:2.2562251851192183\n",
            "train loss:2.2572255868613893\n",
            "train loss:2.257110330945625\n",
            "train loss:2.260624573589717\n",
            "train loss:2.2590106340278933\n",
            "train loss:2.2636764925582202\n",
            "train loss:2.261171008887374\n",
            "train loss:2.2655448750199003\n",
            "train loss:2.2476206802719467\n",
            "train loss:2.252438335937649\n",
            "train loss:2.2452948138560034\n",
            "train loss:2.256754474360579\n",
            "train loss:2.2551765393042458\n",
            "train loss:2.246532698602511\n",
            "train loss:2.2663188663111167\n",
            "train loss:2.2552982500902825\n",
            "train loss:2.2538898268826317\n",
            "train loss:2.2606685168249867\n",
            "train loss:2.2617335613181853\n",
            "train loss:2.259972719695462\n",
            "train loss:2.2669890713931133\n",
            "train loss:2.2316539915145404\n",
            "train loss:2.2487224421426864\n",
            "train loss:2.244773495798903\n",
            "train loss:2.2539747012720968\n",
            "train loss:2.2457410969897795\n",
            "train loss:2.265227232020486\n",
            "train loss:2.2450508116224004\n",
            "train loss:2.253541196013718\n",
            "train loss:2.2488210975392113\n",
            "train loss:2.2549000646420776\n",
            "train loss:2.23460549242333\n",
            "train loss:2.2402668462054613\n",
            "train loss:2.235022944500438\n",
            "train loss:2.2412443430970184\n",
            "train loss:2.240396527285844\n",
            "train loss:2.246720747550589\n",
            "train loss:2.23858693376671\n",
            "train loss:2.233798304935177\n",
            "train loss:2.2438156382227983\n",
            "train loss:2.250027864362538\n",
            "train loss:2.2367424256189894\n",
            "train loss:2.2434247260341853\n",
            "train loss:2.2388492024223567\n",
            "train loss:2.258760084999724\n",
            "train loss:2.256097796421171\n",
            "train loss:2.2441861493149577\n",
            "train loss:2.24175084391759\n",
            "train loss:2.246807396638591\n",
            "train loss:2.236752142530203\n",
            "train loss:2.229775134021672\n",
            "train loss:2.2372249655964493\n",
            "train loss:2.250620586421353\n",
            "train loss:2.2515621641930745\n",
            "train loss:2.239218009163322\n",
            "train loss:2.219758029694804\n",
            "train loss:2.2529385773200676\n",
            "train loss:2.25015818569827\n",
            "train loss:2.2450423298169855\n",
            "train loss:2.232212132693951\n",
            "train loss:2.2147147372881113\n",
            "train loss:2.240513941658282\n",
            "train loss:2.2434300527292184\n",
            "train loss:2.239530626328433\n",
            "train loss:2.232059150121782\n",
            "train loss:2.237271687242292\n",
            "train loss:2.2423438871660557\n",
            "train loss:2.2285417127015066\n",
            "train loss:2.232257926946836\n",
            "train loss:2.227946058285603\n",
            "train loss:2.2309419427062207\n",
            "train loss:2.230386772841214\n",
            "train loss:2.2115428401455786\n",
            "train loss:2.23033202691816\n",
            "train loss:2.2317737578471633\n",
            "train loss:2.211471022180956\n",
            "train loss:2.2170626861116185\n",
            "train loss:2.2469888107138725\n",
            "train loss:2.2459716180335607\n",
            "train loss:2.233763818165368\n",
            "train loss:2.2227421334002853\n",
            "train loss:2.2456337537173288\n",
            "train loss:2.2177242744867005\n",
            "train loss:2.2520095136054126\n",
            "train loss:2.204015800883654\n",
            "train loss:2.246930696968464\n",
            "train loss:2.226316993051637\n",
            "train loss:2.208514029524725\n",
            "train loss:2.228335632646389\n",
            "train loss:2.2229835831081424\n",
            "train loss:2.232668352699485\n",
            "train loss:2.213540567204404\n",
            "train loss:2.2056539847480336\n",
            "train loss:2.2396785428535795\n",
            "train loss:2.2355557514011704\n",
            "train loss:2.218696497839646\n",
            "train loss:2.2013250787918013\n",
            "train loss:2.231070037094251\n",
            "train loss:2.215328926219355\n",
            "train loss:2.233139560474464\n",
            "train loss:2.2172032502635335\n",
            "train loss:2.2212424455928965\n",
            "train loss:2.2214669393005995\n",
            "train loss:2.2336980420537014\n",
            "train loss:2.2125164218304256\n",
            "train loss:2.22096189883941\n",
            "train loss:2.203801009188001\n",
            "train loss:2.2195091159689384\n",
            "train loss:2.236767610870005\n",
            "train loss:2.2031792234827114\n",
            "train loss:2.202295984143025\n",
            "train loss:2.2041666445510866\n",
            "train loss:2.2000003021509036\n",
            "train loss:2.2009529786394877\n",
            "train loss:2.1992948710596094\n",
            "train loss:2.2278639452744033\n",
            "train loss:2.2185829857170747\n",
            "train loss:2.237731835055701\n",
            "train loss:2.2044126509761592\n",
            "train loss:2.2225956825221034\n",
            "train loss:2.2248306650424357\n",
            "train loss:2.2245375691768277\n",
            "train loss:2.2058529289804927\n",
            "train loss:2.1947556992230144\n",
            "train loss:2.1782500360444272\n",
            "train loss:2.248451801390261\n",
            "train loss:2.206757865094315\n",
            "train loss:2.2386542577938027\n",
            "train loss:2.188113802443043\n",
            "train loss:2.2150473508518003\n",
            "train loss:2.245284798213855\n",
            "train loss:2.2033736917421174\n",
            "train loss:2.207295262184976\n",
            "train loss:2.2114657404769327\n",
            "train loss:2.1979090087532884\n",
            "train loss:2.179508030289531\n",
            "train loss:2.166931851699402\n",
            "train loss:2.217197372047662\n",
            "train loss:2.241488998552805\n",
            "train loss:2.2208079175748323\n",
            "train loss:2.2182366091009458\n",
            "train loss:2.2115016605169604\n",
            "train loss:2.204519398273113\n",
            "train loss:2.180617748164078\n",
            "train loss:2.2060270601500296\n",
            "train loss:2.1914963597987103\n",
            "train loss:2.1989819496191476\n",
            "train loss:2.200119556867398\n",
            "train loss:2.2195815678397084\n",
            "train loss:2.16779144274052\n",
            "train loss:2.159918533902693\n",
            "train loss:2.18807038026672\n",
            "train loss:2.2380729501197987\n",
            "train loss:2.1954496182579337\n",
            "train loss:2.1810936344084633\n",
            "train loss:2.1943020798650252\n",
            "train loss:2.176608446652587\n",
            "train loss:2.190789491970271\n",
            "train loss:2.164938536697231\n",
            "train loss:2.1782972920416306\n",
            "train loss:2.2118522060624413\n",
            "train loss:2.1891096046211427\n",
            "train loss:2.217264307600965\n",
            "train loss:2.167694183769921\n",
            "train loss:2.160936149862055\n",
            "train loss:2.195670913645018\n",
            "train loss:2.166920137602014\n",
            "train loss:2.1753946130993524\n",
            "train loss:2.2022150351152097\n",
            "train loss:2.184265446323964\n",
            "train loss:2.1899296855769546\n",
            "train loss:2.1669712378941095\n",
            "train loss:2.179750044758131\n",
            "train loss:2.19411149865077\n",
            "train loss:2.1769864184569765\n",
            "train loss:2.1755557939972316\n",
            "train loss:2.1742916478372862\n",
            "train loss:2.1881594340126465\n",
            "train loss:2.17820626671042\n",
            "train loss:2.198841219933572\n",
            "train loss:2.1708756052806715\n",
            "train loss:2.1737814464510605\n",
            "train loss:2.187950046282799\n",
            "train loss:2.156429351266528\n",
            "train loss:2.183914194489318\n",
            "train loss:2.182708070473593\n",
            "train loss:2.1465375661815953\n",
            "train loss:2.185297295314037\n",
            "train loss:2.1557347403266234\n",
            "train loss:2.1543034437351882\n",
            "train loss:2.191566185807382\n",
            "train loss:2.1449787037653127\n",
            "train loss:2.168306350502485\n",
            "train loss:2.147039030702911\n",
            "train loss:2.1498122745651447\n",
            "train loss:2.180205010134448\n",
            "train loss:2.1464865807886606\n",
            "train loss:2.164184062385286\n",
            "train loss:2.158973137450385\n",
            "train loss:2.1122012036558067\n",
            "train loss:2.1723868514764484\n",
            "train loss:2.1700782844732234\n",
            "train loss:2.1677680669716053\n",
            "train loss:2.1384804118098155\n",
            "train loss:2.1123011604770077\n",
            "train loss:2.168593097017696\n",
            "train loss:2.166989963291071\n",
            "train loss:2.141800482906503\n",
            "train loss:2.164830417093109\n",
            "train loss:2.141853924831924\n",
            "train loss:2.173820980847042\n",
            "train loss:2.1402938194428827\n",
            "train loss:2.1138837870003093\n",
            "train loss:2.149984533946376\n",
            "train loss:2.157465920469726\n",
            "train loss:2.1305546043649373\n",
            "train loss:2.1452113724524127\n",
            "train loss:2.1434687377181074\n",
            "train loss:2.113333178416627\n",
            "train loss:2.149430271628657\n",
            "train loss:2.11297451684051\n",
            "train loss:2.1142152854813165\n",
            "train loss:2.1493693533012235\n",
            "train loss:2.152205701923918\n",
            "train loss:2.179981284646198\n",
            "train loss:2.1351130551421944\n",
            "train loss:2.1571128735007408\n",
            "train loss:2.0939870360981327\n",
            "train loss:2.113432873333286\n",
            "train loss:2.1550619039527974\n",
            "train loss:2.1111693020742526\n",
            "train loss:2.1163220413279076\n",
            "train loss:2.0979996231003897\n",
            "train loss:2.1465885062595693\n",
            "train loss:2.141780744710759\n",
            "train loss:2.1733847037788774\n",
            "train loss:2.1128928029117535\n",
            "train loss:2.10197468032807\n",
            "train loss:2.080357221107599\n",
            "train loss:2.140180503855991\n",
            "train loss:2.0735391244630703\n",
            "train loss:2.0940531636560733\n",
            "train loss:2.124704742325588\n",
            "train loss:2.04806942399107\n",
            "train loss:2.105618650311826\n",
            "train loss:2.0816208715934787\n",
            "train loss:2.1701814296616577\n",
            "train loss:2.1011714731179882\n",
            "train loss:2.1351932590091995\n",
            "train loss:2.1431718558220267\n",
            "train loss:2.1181646381137504\n",
            "train loss:2.055787616684675\n",
            "train loss:2.1147208530231767\n",
            "train loss:2.153363477690024\n",
            "train loss:2.0966157031854364\n",
            "train loss:2.0527393160603222\n",
            "train loss:2.0948404873920183\n",
            "train loss:2.133350838953616\n",
            "train loss:2.0688670362566595\n",
            "train loss:2.059745085070273\n",
            "train loss:2.1059936251835505\n",
            "train loss:2.1386307090038206\n",
            "train loss:2.096756127041343\n",
            "train loss:2.1112486498654928\n",
            "train loss:2.126952177408741\n",
            "train loss:2.1253151709990963\n",
            "train loss:2.1046123427614694\n",
            "train loss:2.090408268832717\n",
            "train loss:2.0660017192293285\n",
            "train loss:2.0508739278625723\n",
            "train loss:2.0549982709023458\n",
            "train loss:2.10511009804983\n",
            "train loss:2.1723932454587076\n",
            "train loss:2.0819801083131733\n",
            "train loss:2.152995778469619\n",
            "train loss:2.1229953590343134\n",
            "train loss:2.123078045753195\n",
            "train loss:2.0771987184991167\n",
            "train loss:2.064694839709071\n",
            "train loss:2.006115792440944\n",
            "train loss:2.089193141728626\n",
            "train loss:2.0786481899898006\n",
            "train loss:2.0346057583416535\n",
            "train loss:2.0161581860849425\n",
            "train loss:2.03728646752067\n",
            "train loss:2.0558983561008115\n",
            "train loss:2.0790197649938964\n",
            "train loss:2.082944719962792\n",
            "train loss:2.0300451029350968\n",
            "train loss:2.061883362959691\n",
            "train loss:1.9894655228688725\n",
            "train loss:1.955384317779155\n",
            "train loss:2.0932224683171747\n",
            "train loss:2.0506822468727215\n",
            "train loss:1.9985416549898263\n",
            "train loss:2.0635409395124005\n",
            "train loss:2.0826270087030903\n",
            "train loss:2.045115861780204\n",
            "train loss:2.0577824070589585\n",
            "train loss:1.9611251436521004\n",
            "train loss:2.1150195766122817\n",
            "train loss:2.059711955027625\n",
            "train loss:1.9900613670285126\n",
            "train loss:1.980358548999663\n",
            "train loss:2.0131966662119227\n",
            "train loss:2.035168495079922\n",
            "train loss:2.022840635921081\n",
            "train loss:2.0928198575108654\n",
            "train loss:1.964369932041037\n",
            "train loss:2.0285552542084075\n",
            "train loss:2.078297848305712\n",
            "train loss:2.0273335510971147\n",
            "train loss:2.0688673568763782\n",
            "train loss:1.9448535789245291\n",
            "train loss:2.0165198197984653\n",
            "train loss:2.0609352182077063\n",
            "train loss:2.0465858493436078\n",
            "train loss:2.0292759550525985\n",
            "train loss:2.0590910585054885\n",
            "train loss:2.0251795935525654\n",
            "train loss:2.0204479600619956\n",
            "train loss:2.0125577574086524\n",
            "train loss:1.9910939878497924\n",
            "train loss:2.087928452473434\n",
            "train loss:1.9910545168626805\n",
            "train loss:2.0920367814508873\n",
            "train loss:2.0318636634210803\n",
            "train loss:1.9687800599123475\n",
            "train loss:2.0473013840412477\n",
            "train loss:1.9726643627551999\n",
            "train loss:1.987857804994928\n",
            "train loss:2.000953648710132\n",
            "train loss:2.0720276249343144\n",
            "train loss:2.063211562822315\n",
            "train loss:2.000985044159088\n",
            "train loss:1.9944987307531465\n",
            "train loss:1.9638951897042731\n",
            "train loss:1.9456846936892163\n",
            "train loss:1.9603026689875926\n",
            "train loss:2.1033777672398433\n",
            "train loss:2.035752845618076\n",
            "train loss:1.8965998605479755\n",
            "train loss:1.9966848870673866\n",
            "train loss:1.8955963574657966\n",
            "train loss:2.023334521682008\n",
            "train loss:1.9510733811028391\n",
            "train loss:1.985659315835361\n",
            "train loss:2.0145865731509707\n",
            "train loss:1.9317088035305512\n",
            "train loss:1.9514846743243424\n",
            "train loss:1.962018199025379\n",
            "train loss:1.949268931786749\n",
            "train loss:2.003191191542246\n",
            "train loss:1.991279587571293\n",
            "train loss:1.9873501778640779\n",
            "train loss:1.9507777154929509\n",
            "train loss:2.001202187811881\n",
            "train loss:1.9538730559937016\n",
            "train loss:2.0577345818175523\n",
            "train loss:1.987278134067571\n",
            "train loss:1.8897596255435627\n",
            "train loss:2.0592305795103614\n",
            "train loss:2.035670849627093\n",
            "train loss:1.9623959109614557\n",
            "train loss:2.0024397699265286\n",
            "train loss:1.9446551405179924\n",
            "train loss:2.023526775516174\n",
            "train loss:1.883321679735892\n",
            "train loss:1.9640973040742318\n",
            "train loss:1.9435085539597896\n",
            "train loss:1.9523884598251422\n",
            "train loss:2.0214181430075566\n",
            "train loss:1.9399447114633472\n",
            "train loss:1.8724187832017802\n",
            "train loss:1.9602221307383394\n",
            "train loss:1.9454283305879363\n",
            "train loss:1.9302128428292311\n",
            "train loss:1.9684045620830346\n",
            "train loss:1.9726920045660075\n",
            "train loss:1.865193400950551\n",
            "train loss:1.9688614371300883\n",
            "train loss:1.9086328644962398\n",
            "train loss:1.797509545717988\n",
            "train loss:1.8328175380748004\n",
            "train loss:1.8976681570155187\n",
            "train loss:1.9216489425875622\n",
            "train loss:1.920370818007944\n",
            "train loss:1.9786161286715072\n",
            "train loss:1.867483611134083\n",
            "train loss:1.8678466909275966\n",
            "train loss:1.9417243520700425\n",
            "train loss:1.9638305218465735\n",
            "train loss:1.745802323435437\n",
            "train loss:1.8275212164832786\n",
            "train loss:1.929357148894824\n",
            "train loss:1.8515248185250597\n",
            "train loss:1.9077041281055778\n",
            "train loss:1.8968675007644626\n",
            "train loss:1.9073605648062055\n",
            "train loss:1.9086422179719584\n",
            "train loss:1.868576275490555\n",
            "train loss:1.9327947181243739\n",
            "train loss:1.8608070677366928\n",
            "train loss:1.8761451913175309\n",
            "train loss:1.915399305615174\n",
            "train loss:1.9186216806590384\n",
            "train loss:1.9363626139024932\n",
            "train loss:1.927858466776711\n",
            "train loss:1.8178490989416087\n",
            "train loss:1.8882326708900363\n",
            "train loss:1.8430239646333753\n",
            "train loss:1.8798853756806022\n",
            "train loss:1.9047111692690135\n",
            "train loss:1.9050929699509067\n",
            "train loss:1.7808655278945158\n",
            "train loss:1.8404278772721117\n",
            "train loss:1.8307804910268521\n",
            "train loss:1.8238541339047427\n",
            "train loss:1.8389983954777458\n",
            "train loss:1.8317752012510748\n",
            "train loss:1.8375562976859896\n",
            "train loss:1.912855450900525\n",
            "train loss:1.7353948923430367\n",
            "train loss:1.8361643038449673\n",
            "train loss:1.8915896752943195\n",
            "train loss:1.8898398913101138\n",
            "train loss:1.939562796806008\n",
            "train loss:1.7894604890940085\n",
            "train loss:1.9186561001621456\n",
            "train loss:1.76850609423221\n",
            "train loss:1.8858057351707387\n",
            "train loss:1.805217878797898\n",
            "train loss:1.9074640025241403\n",
            "train loss:1.862152247308575\n",
            "train loss:1.809214646155635\n",
            "train loss:1.832412332859611\n",
            "train loss:1.896790545091929\n",
            "train loss:1.869615408830756\n",
            "train loss:1.7768356780448997\n",
            "train loss:1.7299513992821252\n",
            "train loss:1.8152161055853389\n",
            "train loss:1.8136925782705142\n",
            "train loss:1.8695678404892475\n",
            "train loss:1.8068508147873188\n",
            "train loss:1.8571851627977438\n",
            "train loss:1.8955084950190488\n",
            "train loss:1.7548146300834535\n",
            "train loss:1.8180551467075379\n",
            "train loss:1.9119170844868418\n",
            "train loss:1.7469046999850684\n",
            "train loss:1.9062138417474035\n",
            "train loss:1.7084068120279396\n",
            "train loss:1.7617301179169125\n",
            "train loss:1.8755455952974822\n",
            "train loss:1.85147250736691\n",
            "train loss:1.7707269812433086\n",
            "train loss:1.78981178256702\n",
            "train loss:1.7586364951867606\n",
            "train loss:1.8067529966805962\n",
            "train loss:1.8777656607061357\n",
            "train loss:1.7159580094365523\n",
            "train loss:1.7927425476365217\n",
            "train loss:1.9065620252444546\n",
            "train loss:1.8391741748635284\n",
            "train loss:1.7946131220694974\n",
            "train loss:1.7976762109581212\n",
            "train loss:1.8754866988693237\n",
            "train loss:1.8167623689424695\n",
            "train loss:1.7034154681855265\n",
            "train loss:1.8636603905039317\n",
            "train loss:1.6736373465867644\n",
            "train loss:1.7643865654077375\n",
            "train loss:1.7185140659249685\n",
            "train loss:1.8893679633475187\n",
            "train loss:1.7288386468489023\n",
            "train loss:1.7314557441344411\n",
            "train loss:1.927075594961809\n",
            "train loss:1.8716568166508063\n",
            "train loss:1.8090155151972789\n",
            "train loss:1.7561221958813666\n",
            "train loss:1.7296803258166777\n",
            "train loss:1.6894817262586235\n",
            "train loss:1.7521513355792797\n",
            "train loss:1.7449158324938994\n",
            "train loss:1.6979150374748695\n",
            "train loss:1.8891419593534655\n",
            "train loss:1.6875855123306807\n",
            "train loss:1.7756730054300516\n",
            "train loss:1.820135594009835\n",
            "train loss:1.7472576540362244\n",
            "train loss:1.6988777526390584\n",
            "train loss:1.7984426777570655\n",
            "train loss:1.7796237916224424\n",
            "train loss:1.7285822372061546\n",
            "train loss:1.8276376536552752\n",
            "train loss:1.83087862105325\n",
            "train loss:1.8105913506592664\n",
            "train loss:1.723278889017968\n",
            "train loss:1.785636200817239\n",
            "train loss:1.8569826032108643\n",
            "train loss:1.7437905971283212\n",
            "train loss:1.6753242442738578\n",
            "train loss:1.8180141446739173\n",
            "train loss:1.75268602846128\n",
            "train loss:1.7829584379162466\n",
            "train loss:1.6674017904870089\n",
            "train loss:1.7229872384473506\n",
            "train loss:1.728900185523039\n",
            "train loss:1.7636781388867915\n",
            "train loss:1.6036884691794981\n",
            "train loss:1.6615488886385983\n",
            "train loss:1.7865043401329859\n",
            "train loss:1.718823209405146\n",
            "train loss:1.7011429351657172\n",
            "train loss:1.6410590280260615\n",
            "train loss:1.6932100398912027\n",
            "train loss:1.6478379676108637\n",
            "train loss:1.6194589504510224\n",
            "train loss:1.6976344845072942\n",
            "train loss:1.7188850810166094\n",
            "train loss:1.6152478842963403\n",
            "train loss:1.5842405519015978\n",
            "train loss:1.6962336185190008\n",
            "train loss:1.6408973483823381\n",
            "train loss:1.7044754239527709\n",
            "train loss:1.6682323814702824\n",
            "train loss:1.6672195646642984\n",
            "train loss:1.655400987518924\n",
            "train loss:1.7274208703991385\n",
            "train loss:1.7221920642784934\n",
            "train loss:1.70950650964809\n",
            "train loss:1.6212490477121724\n",
            "train loss:1.7994892501828323\n",
            "train loss:1.6867423540774624\n",
            "train loss:1.7135337446130512\n",
            "train loss:1.6815668058927609\n",
            "train loss:1.5889718883080388\n",
            "train loss:1.7110799658148013\n",
            "train loss:1.6049963606716775\n",
            "train loss:1.6167391427808075\n",
            "train loss:1.5106980988934517\n",
            "train loss:1.499422963230739\n",
            "train loss:1.661770010379714\n",
            "train loss:1.7511043580405095\n",
            "train loss:1.6479344750989267\n",
            "train loss:1.6368906022561496\n",
            "train loss:1.5263832850665167\n",
            "train loss:1.7322253455680494\n",
            "train loss:1.6405865849954815\n",
            "train loss:1.5945198408665278\n",
            "train loss:1.6856824085375102\n",
            "train loss:1.7195965735043324\n",
            "train loss:1.55732128768142\n",
            "train loss:1.556792137142183\n",
            "train loss:1.6123587139299547\n",
            "train loss:1.6702650170448665\n",
            "train loss:1.531092319776828\n",
            "train loss:1.4873758988623278\n",
            "train loss:1.6111578808689142\n",
            "train loss:1.6656689835552132\n",
            "train loss:1.5964815155509877\n",
            "train loss:1.618088583867996\n",
            "train loss:1.5897263580600318\n",
            "train loss:1.4687126902653906\n",
            "train loss:1.7772062461771734\n",
            "train loss:1.6657856346589963\n",
            "train loss:1.623125559147621\n",
            "train loss:1.6114091613702808\n",
            "train loss:1.5800283947372433\n",
            "train loss:1.7053637432112152\n",
            "train loss:1.6311968768413478\n",
            "train loss:1.6292517906915556\n",
            "train loss:1.6684296533913408\n",
            "train loss:1.6099699632657554\n",
            "train loss:1.5473107647327282\n",
            "train loss:1.6715340964639267\n",
            "train loss:1.638658320248443\n",
            "train loss:1.5774845425933979\n",
            "train loss:1.6322851960903202\n",
            "train loss:1.5469977675926423\n",
            "train loss:1.5824299399194472\n",
            "train loss:1.561164124078124\n",
            "train loss:1.5792145671015947\n",
            "train loss:1.5158070260917476\n",
            "train loss:1.5809722942885551\n",
            "train loss:1.5189325230304518\n",
            "train loss:1.6484189835358487\n",
            "train loss:1.6502393190307965\n",
            "train loss:1.7059189280360985\n",
            "train loss:1.5140624426643514\n",
            "train loss:1.5859095030608878\n",
            "train loss:1.5907548366222855\n",
            "train loss:1.475285952172578\n",
            "train loss:1.6398319981185165\n",
            "train loss:1.5878794393210276\n",
            "train loss:1.565737505291057\n",
            "train loss:1.6184463908550568\n",
            "train loss:1.4946520972348956\n",
            "train loss:1.457461593706889\n",
            "train loss:1.455254673888816\n",
            "train loss:1.6179662852832308\n",
            "train loss:1.4912760757030372\n",
            "train loss:1.5811183016420105\n",
            "train loss:1.585031940789305\n",
            "train loss:1.5958183586837962\n",
            "train loss:1.4186986983022687\n",
            "train loss:1.4462654109307609\n",
            "train loss:1.4630970322640655\n",
            "train loss:1.3913457302123726\n",
            "train loss:1.5869262359922505\n",
            "train loss:1.574465447551189\n",
            "train loss:1.548781201949342\n",
            "train loss:1.4632097992782436\n",
            "train loss:1.5227887519664676\n",
            "train loss:1.416474780557015\n",
            "train loss:1.4394338986906419\n",
            "train loss:1.495159453678549\n",
            "train loss:1.475396550721201\n",
            "train loss:1.4024249396691757\n",
            "train loss:1.5267949681411297\n",
            "train loss:1.5486842540462684\n",
            "train loss:1.4102028321211535\n",
            "train loss:1.5707466185102659\n",
            "train loss:1.536772368573129\n",
            "train loss:1.476120746858205\n",
            "train loss:1.6436832774285433\n",
            "train loss:1.5224682447762345\n",
            "train loss:1.4035620845720347\n",
            "train loss:1.5288161802255942\n",
            "train loss:1.4954584774163804\n",
            "train loss:1.3825364127783617\n",
            "train loss:1.396964909777215\n",
            "train loss:1.394426846383039\n",
            "train loss:1.5459249219979196\n",
            "train loss:1.3336974906916208\n",
            "train loss:1.582323558580388\n",
            "train loss:1.377269261959713\n",
            "train loss:1.4271368624695733\n",
            "train loss:1.4065308117938624\n",
            "train loss:1.4990356757873406\n",
            "train loss:1.3760403754848816\n",
            "train loss:1.586347555888356\n",
            "train loss:1.3700736948725234\n",
            "train loss:1.4691336180261225\n",
            "train loss:1.3956600729722806\n",
            "train loss:1.4919269816355878\n",
            "train loss:1.3204866334907748\n",
            "train loss:1.4019899326865426\n",
            "train loss:1.5018599658839955\n",
            "train loss:1.4831004828540872\n",
            "train loss:1.3877830106061908\n",
            "train loss:1.5025774239178145\n",
            "train loss:1.4211285882651623\n",
            "train loss:1.532828363787634\n",
            "train loss:1.4859736196928912\n",
            "train loss:1.5018512363446745\n",
            "train loss:1.4979926460469906\n",
            "train loss:1.3213887016988315\n",
            "train loss:1.3250195955170816\n",
            "train loss:1.2996189596319043\n",
            "train loss:1.4364789303845695\n",
            "train loss:1.4727095828704702\n",
            "train loss:1.5059919297982705\n",
            "train loss:1.4143158969240084\n",
            "train loss:1.4381141046115125\n",
            "train loss:1.4624963706151144\n",
            "train loss:1.3891807337224549\n",
            "train loss:1.3874280738272389\n",
            "train loss:1.35631500725935\n",
            "train loss:1.4053388708177734\n",
            "train loss:1.4992121811493657\n",
            "train loss:1.402032540447481\n",
            "train loss:1.375689589730194\n",
            "train loss:1.251442234908419\n",
            "train loss:1.4961286761332582\n",
            "train loss:1.4644719909029018\n",
            "train loss:1.5809571545520429\n",
            "train loss:1.4295944869412125\n",
            "train loss:1.3217931862205443\n",
            "train loss:1.419913523724024\n",
            "train loss:1.3276778451457087\n",
            "train loss:1.4124213351891421\n",
            "train loss:1.1512092843825144\n",
            "train loss:1.3262128132291429\n",
            "train loss:1.3654225708481702\n",
            "train loss:1.4300137784694829\n",
            "train loss:1.296013071295575\n",
            "train loss:1.5087996836461792\n",
            "train loss:1.2567404264445219\n",
            "train loss:1.3776380925423468\n",
            "train loss:1.3636976923385646\n",
            "train loss:1.3888368560961308\n",
            "train loss:1.4249599016694865\n",
            "train loss:1.2753817881270997\n",
            "train loss:1.4739681679836978\n",
            "train loss:1.2455309403465398\n",
            "train loss:1.398207455469281\n",
            "train loss:1.3387699252662093\n",
            "train loss:1.45281973835157\n",
            "train loss:1.3812923016661787\n",
            "train loss:1.4841974106825526\n",
            "train loss:1.4537963531180265\n",
            "train loss:1.2973631845411293\n",
            "train loss:1.3592816348077952\n",
            "train loss:1.1550484090915312\n",
            "train loss:1.3532020073260052\n",
            "train loss:1.1885976422382898\n",
            "train loss:1.3854778261185918\n",
            "train loss:1.3431919045523073\n",
            "train loss:1.377056841639727\n",
            "train loss:1.23063928048693\n",
            "train loss:1.373104939445086\n",
            "train loss:1.2510747482731015\n",
            "train loss:1.4136703893485616\n",
            "train loss:1.4213581601757737\n",
            "train loss:1.2427171617652824\n",
            "train loss:1.2581108202920306\n",
            "train loss:1.3260592772452273\n",
            "train loss:1.2640407917919938\n",
            "train loss:1.4510070591761606\n",
            "train loss:1.339299522064855\n",
            "train loss:1.403674330523498\n",
            "train loss:1.2477438683800148\n",
            "train loss:1.3517986975900118\n",
            "train loss:1.3712117068425655\n",
            "train loss:1.1909453711906552\n",
            "train loss:1.3280467308710382\n",
            "train loss:1.3194638368579508\n",
            "train loss:1.3005329743739\n",
            "train loss:1.1670694235877905\n",
            "train loss:1.0869946752558761\n",
            "train loss:1.3217868832409083\n",
            "train loss:1.338392565570882\n",
            "train loss:1.1729222331208795\n",
            "train loss:1.2473258568367103\n",
            "train loss:1.2658988027999947\n",
            "train loss:1.288915010216206\n",
            "train loss:1.1814155639331054\n",
            "train loss:1.3996884607622269\n",
            "train loss:1.2495492096871779\n",
            "train loss:1.1646390799234119\n",
            "train loss:1.2750300931371115\n",
            "train loss:1.261968886395997\n",
            "train loss:1.2458643145902613\n",
            "train loss:1.3923494340821168\n",
            "train loss:1.2126945408219414\n",
            "train loss:1.247863249469725\n",
            "train loss:1.3351225183950624\n",
            "train loss:1.268147561513593\n",
            "train loss:1.1590036248013167\n",
            "train loss:1.348712774255634\n",
            "train loss:1.4175045267925643\n",
            "train loss:1.3057902275467945\n",
            "train loss:1.3153107696052053\n",
            "train loss:1.329941958763651\n",
            "train loss:1.310181294024982\n",
            "train loss:1.272405361121194\n",
            "train loss:1.1269811571270776\n",
            "train loss:1.2137980302434896\n",
            "train loss:1.1670494879198179\n",
            "train loss:1.06879227158542\n",
            "train loss:1.3907860195600237\n",
            "train loss:1.2404583669169096\n",
            "train loss:1.1960754712630133\n",
            "train loss:1.2235138075637502\n",
            "train loss:1.3759559563040509\n",
            "train loss:1.2934454236473494\n",
            "train loss:1.2697715080789458\n",
            "train loss:1.1078526875794292\n",
            "train loss:1.1703957316984013\n",
            "train loss:1.242960808449355\n",
            "train loss:1.273392034994815\n",
            "train loss:1.4373201173960177\n",
            "train loss:1.301054040662038\n",
            "train loss:1.3027005719568983\n",
            "train loss:1.2202716690040472\n",
            "train loss:1.1772639320034217\n",
            "train loss:1.1938439204922642\n",
            "train loss:1.2040393631878878\n",
            "train loss:1.2699909381404948\n",
            "train loss:1.1706271571300144\n",
            "train loss:1.261118910697126\n",
            "train loss:1.2135801315727432\n",
            "train loss:1.2443963681554084\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#그래프에서 보면\n",
        "\n",
        "++ 아무 조치도 취하지 않은 테스트 그래프는 가중치 감소의 그래프의 두 간격보다 훨씬 벌어져 있습니다.\n",
        "\n",
        "\n",
        "훈련데이터와 시험 데이터에 대한 정확도 차이가 현저히 줄었다.\n",
        "\n",
        "또 훈련데이터에 대한 정확도가 100%에 도달하지도 않게 되었다.\n",
        "\n",
        "+ 이 뜻은 , 안좋은 의미가 아니라 그만큼 표현력이 높아졌다고 해석 가능\n",
        "\n",
        "+ **드롭아웃은 앙상블 학습과 같은 효과를 하나의 네트워크로 구현했**다고 해석 가능\n",
        "\n",
        "\n",
        "-------> ex) 드롭아웃으로 학습 때 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석 가능\n",
        "\n",
        "-------> ex) (안하지만) 추론(시험)때 뉴런의 출력에 삭제한 비율을 곱하는 방식은 앙상블  학습에서 여러 모델의 평균을 내는 것으로 해석 가능\n",
        "\n",
        "+ 앙상블 학습 : 개별적으로  학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식\n",
        "\n",
        "\n",
        "드롭아웃이나 가중치 감소나 일종의 layer 계층으로 활동합니다\n",
        "\n"
      ],
      "metadata": {
        "id": "qFHxJHzoQ3_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#적절한 하이퍼파라미터 값 찾기\n",
        "\n",
        "하이퍼 파라미터를 다양한 값으로 설정하고 검증할때, 시험 데이터를 사용해서는 안된다. 나중에 테스트를 그걸로 할건데 그걸로 하면 어캄. \n",
        "\n",
        "그래서 따로 데이터가 필요한데, 이 데이터를 검증 데이터라고 한다.\n",
        "\n",
        "**훈련 데이터 : 매개변수 학습**\n",
        "\n",
        "**검증 데이터 : 하이퍼파라미터 성능 평가**\n",
        "\n",
        "**시험 데이터 : 신경망의 범용 성능 평가**\n",
        "\n",
        "#하이퍼 파라미터 최적화\n",
        "\n",
        "**핵심 : 하이퍼 파라미터의 최적 값이 존재하는 범위를 조금씩 줄여나가기**\n",
        "\n",
        "참고로 그리드 서치같은 규칙적인 탐색보다는 무작위 탐색이 좋은 결과를 낸다고  알려져있다\n",
        "\n",
        "하이퍼 파라미터의 최적화의 딥러닝 학습에는 오랜시간이 걸림(거의 몇 주 이상) .\n",
        "\n",
        "그래서 학습을 위한 에폭을 작게 하여 1회 평가에 걸리는 시간을 단축하는 것이 효과적. \n",
        "\n",
        "또한 하이퍼 파라미터의 범위는 대략적으로 지정하는 것이 효과적임. \n",
        "\n",
        "**대략적**으로 지정하고, 그 범위에서 **무작위**로 하이퍼파라미터의 값을 골라내고 그 값으로 **정확도를 평가하고** \n",
        "\n",
        "이러한 작업을 **여러번 반복**하여 **하이퍼 파라미터의 최적값의 범위를 좁혀나가**는 것이다\n",
        "\n",
        "이는 <실용적인 방법> 이고, 세련된 기법으로는 <베이즈 최적화>가 있긴 하다\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "swJ26r6pmg_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#하이퍼 파라미터 최적화 구현 (코드 생략)\n",
        "\n",
        "MNIST 데이터셋에서, 학습률과 가중치감소계수 이 2개의 하이퍼 파라미터를 탐색하는 문제\n",
        "\n",
        "앞서 말한대로 하이퍼 파라미터의 검증은 예를 들어 0.001~1000같은 로그 스케일 범위에서 무작위로 추출해 수행한다.\n",
        "\n",
        "이 예에서는 가중치 감소 계수를 10^-8 ~ 10^-4, 학습률을 10^-6 ~ 10^-2 범위에서 시작한다. "
      ],
      "metadata": {
        "id": "4po4Wv-iuR1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "weight_key = 10 ** np.random.uniform(-8,-4)\n",
        "lr  =  10**np.random.uniform(-6,-2)\n",
        "#np.random.uniform(low,high,size) : 균등분포로부터 low~high범위내 무작위 size번 추출. 여기서는 default로 1개가 들어간 것으로 보임  "
      ],
      "metadata": {
        "id": "bzSpNlCvP4F_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#이처럼\n",
        "\n",
        "무작위로 추출한 값을 사용하여 학습을 수행. 이러한 작업을 여러차례 학습 반복하여 신경망에 좋을 것 같은 값이 어디에 존재하는지 관찰\n",
        "\n",
        "#결과를 보면\n",
        "\n",
        "당연하게도 어떠한 값은 정확도가 매우 높은 반면 어떠한 값들은 학습이 잘 되지 않는다.\n",
        "\n",
        "학습이 잘 진행 때의 학습률은 0.001~0.01, \n",
        "\n",
        "가중치 감소 계수는 10^-8~ 10^-6 정도라는 것을 알 수 있다.\n",
        "\n",
        "이처럼 잘될 것 같은 값의 범위를 좁혀나간다. 이후 그 축소된 범위로 똑같은 작업을 반복한다.\n",
        "\n",
        "이렇게 적절한 값이 위치한 범위를 좁혀가다가 특정 단계에서 최종 하이퍼파라미터 값을 하나 선택한다."
      ],
      "metadata": {
        "id": "uK3Ibh0XxdkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NeksmX4Zw86-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}